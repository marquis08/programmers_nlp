{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from time import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "import datetime\n",
    "import pkg_resources\n",
    "#import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import gc\n",
    "import re\n",
    "import operator \n",
    "import sys\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, Subset, DataLoader\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "#from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "#%matplotlib inline\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import os\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "import pickle\n",
    "#from apex import amp\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=123):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class AdamW(Optimizer):\n",
    "    \"\"\"Implements AdamW algorithm.\n",
    "\n",
    "    It has been proposed in `Fixing Weight Decay Regularization in Adam`_.\n",
    "\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float, optional): learning rate (default: 1e-3)\n",
    "        betas (Tuple[float, float], optional): coefficients used for computing\n",
    "            running averages of gradient and its square (default: (0.9, 0.999))\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability (default: 1e-8)\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "\n",
    "    .. Fixing Weight Decay Regularization in Adam:\n",
    "    https://arxiv.org/abs/1711.05101\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay)\n",
    "        super(AdamW, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('AdamW does not support sparse gradients, please consider SparseAdam instead')\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                # according to the paper, this penalty should come after the bias correction\n",
    "                # if group['weight_decay'] != 0:\n",
    "                #     grad = grad.add(group['weight_decay'], p.data)\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "\n",
    "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p.data.add_(-group['weight_decay'], p.data)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 295 \n",
    "SEED = 42\n",
    "EPOCHS = 20\n",
    "Data_dir=\"../job_nlp/\"\n",
    "WORK_DIR = \"../job_nlp/working/\"\n",
    "#num_to_load=100000                         #Train size to match time limit\n",
    "#valid_size= 50000                          #Validation Size\n",
    "TARGET = 'smishing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/matsuik/ppbert\n",
    "package_dir_a = \"../job_nlp/ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT\"\n",
    "sys.path.insert(0, package_dir_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/home/yilgukseo/anaconda3/envs/pytorch/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n  return f(*args, **kwds)\n/home/yilgukseo/anaconda3/envs/pytorch/lib/python3.7/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n  return f(*args, **kwds)\n/home/yilgukseo/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py:5879: ResourceWarning: unclosed file <_io.TextIOWrapper name='/home/yilgukseo/.keras/keras.json' mode='r' encoding='UTF-8'>\n  _config = json.load(open(_config_path))\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
    }
   ],
   "source": [
    "from pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification,BertAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "erNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_9/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_9/output/dense/kernel with shape [3072, 768]\nLoading TF weight bert/pooler/dense/bias with shape [768]\nLoading TF weight bert/pooler/dense/kernel with shape [768, 768]\nLoading TF weight cls/predictions/output_bias with shape [30522]\nLoading TF weight cls/predictions/transform/LayerNorm/beta with shape [768]\nLoading TF weight cls/predictions/transform/LayerNorm/gamma with shape [768]\nLoading TF weight cls/predictions/transform/dense/bias with shape [768]\nLoading TF weight cls/predictions/transform/dense/kernel with shape [768, 768]\nLoading TF weight cls/seq_relationship/output_bias with shape [2]\nLoading TF weight cls/seq_relationship/output_weights with shape [2, 768]\nInitialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\nInitialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\nInitialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\nInitialize PyTorch weight ['cls', 'predictions', 'output_bias']\nInitialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'bias']\nInitialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'kernel']\nInitialize PyTorch weight ['cls', 'seq_relationship', 'output_bias']\nInitialize PyTorch weight ['cls', 'seq_relationship', 'output_weights']\nSave PyTorch model to ../job_nlp/working/pytorch_model.bin\n"
    },
    {
     "data": {
      "text/plain": "'../job_nlp/working/bert_config.json'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Translate model from tensorflow to pytorch\n",
    "BERT_MODEL_PATH = '../job_nlp/bert-pretrained-models/uncased_L-12_H-768_A-12/uncased_L-12_H-768_A-12/'\n",
    "convert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n",
    "    BERT_MODEL_PATH + 'bert_model.ckpt',\n",
    "BERT_MODEL_PATH + 'bert_config.json',\n",
    "WORK_DIR + 'pytorch_model.bin')\n",
    "\n",
    "shutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', WORK_DIR + 'bert_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the Bert configuration file\n",
    "from pytorch_pretrained_bert import BertConfig\n",
    "\n",
    "bert_config = BertConfig('../job_nlp/bert-pretrained-models/uncased_L-12_H-768_A-12/uncased_L-12_H-768_A-12/'+'bert_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"max_position_embeddings\": 512,\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 30522\n}"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the lines to BERT format\n",
    "# Thanks to https://www.kaggle.com/httpwwwfszyc/bert-in-keras-taming\n",
    "def convert_lines(example, max_seq_length,tokenizer):\n",
    "    max_seq_length -=2\n",
    "    all_tokens = []\n",
    "    longer = 0\n",
    "    for text in tqdm_notebook(example):\n",
    "        tokens_a = tokenizer.tokenize(text)\n",
    "        if len(tokens_a)>max_seq_length:\n",
    "            tokens_a = tokens_a[:max_seq_length]\n",
    "            longer += 1\n",
    "        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n",
    "        all_tokens.append(one_token)\n",
    "    print(longer)\n",
    "    return np.array(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_PATH = '../job_nlp/bert-pretrained-models/uncased_L-12_H-768_A-12/uncased_L-12_H-768_A-12/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 26.6 ms, sys: 3.5 ms, total: 30.1 ms\nWall time: 29.8 ms\n"
    }
   ],
   "source": [
    "%%time\n",
    "train_df = pd.read_csv(os.path.join(Data_dir,\"train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(Data_dir,\"test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "3    0.354552\n5    0.248457\n4    0.184414\n2    0.113812\n1    0.098765\nName: label, dtype: float64"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "title      [$$$] 재귀함수를 처리하면서 setTimeout으로 딜레이를 주면 읽는 순서가 ...\ncontent                                                  NaN\nlabel                                                      4\nName: 2400, dtype: object\n"
    }
   ],
   "source": [
    "# replace NaN\n",
    "print(train_df.iloc[2400,:])\n",
    "train_df.iloc[2400,1] = \"@@@\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 38.2 ms, sys: 0 ns, total: 38.2 ms\nWall time: 37.8 ms\n"
    }
   ],
   "source": [
    "%%time\n",
    "train_df['text'] = train_df[['title', 'content']].apply(lambda x: ' '.join(x), axis = 1)\n",
    "test_df['text'] = test_df[['title', 'content']].apply(lambda x: ' '.join(x), axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>content</th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>wait()과 sleep()의 차이점은 뭔가요</td>\n      <td>###발생하는 문제 및 실행환경\\nwait()과 sleep()의 차이점은 뭔가요</td>\n      <td>3</td>\n      <td>wait()과 sleep()의 차이점은 뭔가요 ###발생하는 문제 및 실행환경\\nw...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>$$$ 초보 외부 클래스 멤버 변수 사용 질문합니다.ㅠㅠ</td>\n      <td>1.헤더에, 사용할 멤버변수가 담긴 헤더 파일이 Include 되어있습니다.\\n예를...</td>\n      <td>2</td>\n      <td>$$$ 초보 외부 클래스 멤버 변수 사용 질문합니다.ㅠㅠ 1.헤더에, 사용할 멤버변...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ORM: Sequelize: 다대다 관계 쿼리</td>\n      <td>안녕하세요.\\n\\n어떻게 다대다 관계 쿼리를 해야하나요? 예를들어, `product...</td>\n      <td>4</td>\n      <td>ORM: Sequelize: 다대다 관계 쿼리 안녕하세요.\\n\\n어떻게 다대다 관계...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>$$$에서 숫자가 NaN인지 검사하려면 어떻게해야하죠?</td>\n      <td>```\\nparseFloat('geoff') == NaN;\\n\\nparseFloat...</td>\n      <td>4</td>\n      <td>$$$에서 숫자가 NaN인지 검사하려면 어떻게해야하죠? ```\\nparseFloat...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>$$$ 2.7에서 중국어, 특수문자 인코딩 하는 방법이 뭔가요?</td>\n      <td>```\\n&gt;&gt;&gt;dict['name']\\n胡安·马塔\\n&gt;&gt;&gt;json.dumps(dic...</td>\n      <td>5</td>\n      <td>$$$ 2.7에서 중국어, 특수문자 인코딩 하는 방법이 뭔가요? ```\\n&gt;&gt;&gt;di...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                 title  \\\n0            wait()과 sleep()의 차이점은 뭔가요   \n1      $$$ 초보 외부 클래스 멤버 변수 사용 질문합니다.ㅠㅠ   \n2            ORM: Sequelize: 다대다 관계 쿼리   \n3       $$$에서 숫자가 NaN인지 검사하려면 어떻게해야하죠?   \n4  $$$ 2.7에서 중국어, 특수문자 인코딩 하는 방법이 뭔가요?   \n\n                                             content  label  \\\n0       ###발생하는 문제 및 실행환경\\nwait()과 sleep()의 차이점은 뭔가요      3   \n1  1.헤더에, 사용할 멤버변수가 담긴 헤더 파일이 Include 되어있습니다.\\n예를...      2   \n2  안녕하세요.\\n\\n어떻게 다대다 관계 쿼리를 해야하나요? 예를들어, `product...      4   \n3  ```\\nparseFloat('geoff') == NaN;\\n\\nparseFloat...      4   \n4  ```\\n>>>dict['name']\\n胡安·马塔\\n>>>json.dumps(dic...      5   \n\n                                                text  \n0  wait()과 sleep()의 차이점은 뭔가요 ###발생하는 문제 및 실행환경\\nw...  \n1  $$$ 초보 외부 클래스 멤버 변수 사용 질문합니다.ㅠㅠ 1.헤더에, 사용할 멤버변...  \n2  ORM: Sequelize: 다대다 관계 쿼리 안녕하세요.\\n\\n어떻게 다대다 관계...  \n3  $$$에서 숫자가 NaN인지 검사하려면 어떻게해야하죠? ```\\nparseFloat...  \n4  $$$ 2.7에서 중국어, 특수문자 인코딩 하는 방법이 뭔가요? ```\\n>>>di...  "
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "25"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.iloc[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "43"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.iloc[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "69"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.iloc[0,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['t_length'] = train_df['title'].apply(lambda x: len(x))\n",
    "train_df['c_length'] = train_df['content'].apply(lambda x: len(x))\n",
    "train_df['text_length'] = train_df['text'].apply(lambda x: len(x))\n",
    "\n",
    "test_df['t_length'] = test_df['title'].apply(lambda x: len(x))\n",
    "test_df['c_length'] = test_df['content'].apply(lambda x: len(x))\n",
    "test_df['text_length'] = test_df['text'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>t_length</th>\n      <th>c_length</th>\n      <th>text_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>2592.000000</td>\n      <td>2592.000000</td>\n      <td>2592.000000</td>\n      <td>2592.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>3.369985</td>\n      <td>29.945988</td>\n      <td>891.706404</td>\n      <td>922.652392</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.245321</td>\n      <td>15.235909</td>\n      <td>1684.607332</td>\n      <td>1685.667323</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>3.000000</td>\n      <td>8.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>3.000000</td>\n      <td>19.000000</td>\n      <td>180.000000</td>\n      <td>208.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>3.000000</td>\n      <td>27.000000</td>\n      <td>385.000000</td>\n      <td>418.500000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>4.000000</td>\n      <td>37.000000</td>\n      <td>896.000000</td>\n      <td>920.750000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>5.000000</td>\n      <td>119.000000</td>\n      <td>28756.000000</td>\n      <td>28780.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "             label     t_length      c_length   text_length\ncount  2592.000000  2592.000000   2592.000000   2592.000000\nmean      3.369985    29.945988    891.706404    922.652392\nstd       1.245321    15.235909   1684.607332   1685.667323\nmin       1.000000     3.000000      3.000000      8.000000\n25%       3.000000    19.000000    180.000000    208.000000\n50%       3.000000    27.000000    385.000000    418.500000\n75%       4.000000    37.000000    896.000000    920.750000\nmax       5.000000   119.000000  28756.000000  28780.000000"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>t_length</th>\n      <th>c_length</th>\n      <th>text_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>500.000000</td>\n      <td>500.000000</td>\n      <td>500.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>29.310000</td>\n      <td>858.270000</td>\n      <td>888.580000</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>14.786046</td>\n      <td>1957.436752</td>\n      <td>1956.747171</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>5.000000</td>\n      <td>20.000000</td>\n      <td>53.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>19.750000</td>\n      <td>192.250000</td>\n      <td>227.250000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>26.500000</td>\n      <td>371.000000</td>\n      <td>399.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>36.000000</td>\n      <td>841.250000</td>\n      <td>870.500000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>134.000000</td>\n      <td>35210.000000</td>\n      <td>35226.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "         t_length      c_length   text_length\ncount  500.000000    500.000000    500.000000\nmean    29.310000    858.270000    888.580000\nstd     14.786046   1957.436752   1956.747171\nmin      5.000000     20.000000     53.000000\n25%     19.750000    192.250000    227.250000\n50%     26.500000    371.000000    399.000000\n75%     36.000000    841.250000    870.500000\nmax    134.000000  35210.000000  35226.000000"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "loaded 2592 records\n/home/yilgukseo/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  import sys\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c1318889d5a4a08852bb4b6205d7d39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=2592.0), HTML(value='')))"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n814\nX_train : 2592\nloaded 500 records\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e942844adb4f44a3b5fcbf43cfdd410a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=500.0), HTML(value='')))"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n150\nX_test : 500\n"
    }
   ],
   "source": [
    "#%%time\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)\n",
    "#train_df = pd.read_csv(os.path.join(Data_dir,\"train.csv\"))#.sample(num_to_load+valid_size,random_state=SEED)\n",
    "print('loaded %d records' % len(train_df))\n",
    "# Make sure all comment_text values are strings\n",
    "train_df['text'] = train_df['text'].astype(str) \n",
    "x_train = convert_lines(train_df[\"text\"].fillna(\"DUMMY_VALUE\"),MAX_SEQUENCE_LENGTH,tokenizer)\n",
    "print(\"X_train : {}\".format(len(x_train)))\n",
    "\n",
    "#test_df = pd.read_csv(os.path.join(Data_dir,\"public_test.csv\"))#.sample(num_to_load+valid_size,random_state=SEED)\n",
    "print('loaded %d records' % len(test_df))\n",
    "test_df['text'] = test_df['text'].astype(str) \n",
    "x_test = convert_lines(test_df[\"text\"].fillna(\"DUMMY_VALUE\"),MAX_SEQUENCE_LENGTH,tokenizer)\n",
    "print(\"X_test : {}\".format(len(x_test)))\n",
    "\n",
    "train_df=train_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# above not working in linux ?? these x_train & x_test are obtained from windows\n",
    "#x_train = np.loadtxt('../job_nlp/x_train.csv', delimiter=',')\n",
    "#x_test = np.loadtxt('../job_nlp/x_test.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(SEED)\n",
    "output_model_file = \"bert_pytorch.bin\"\n",
    "\n",
    "lr=2e-5\n",
    "batch_size = 4\n",
    "accumulation_steps=2\n",
    "n_labels = 2\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "TARGET = 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[TARGET] = train_df[TARGET]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([2, 1, 3,  ..., 0, 4, 1])"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x_train = train_df['text']\n",
    "y_train = torch.tensor(train_df[TARGET])#.long()\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([2, 1, 3, 3, 4])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(x):\n",
    "    return x.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(torch.tensor(x_test, dtype = torch.long)) #TensorDataset(X_valid, valid_length, torch.tensor(Y_valid))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"../job_nlp/working\",cache_dir=None, num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "1/5 fold training starts!\n/home/yilgukseo/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 1 - train_loss: 1.22631  val_loss: 0.92264  val_acc: 0.64547  elapsed: 2m 3s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 2 - train_loss: 0.77406  val_loss: 0.67425  val_acc: 0.75915  elapsed: 4m 4s\n================ ༼ つ ◕_◕ ༽つ Epoch 3 - train_loss: 0.53441  val_loss: 0.65998  val_acc: 0.75915  elapsed: 6m 7s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 4 - train_loss: 0.38394  val_loss: 0.67991  val_acc: 0.77071  elapsed: 8m 8s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 5 - train_loss: 0.28553  val_loss: 0.70645  val_acc: 0.78227  elapsed: 10m 11s\n================ ༼ つ ◕_◕ ༽つ Epoch 6 - train_loss: 0.20799  val_loss: 0.87254  val_acc: 0.78227  elapsed: 12m 14s\n================ ༼ つ ◕_◕ ༽つ Epoch 7 - train_loss: 0.18346  val_loss: 0.97542  val_acc: 0.78227  elapsed: 14m 15s\n================ ༼ つ ◕_◕ ༽つ Epoch 8 - train_loss: 0.16235  val_loss: 0.98278  val_acc: 0.78227  elapsed: 16m 15s\n================ ༼ つ ◕_◕ ༽つ Epoch 9 - train_loss: 0.09885  val_loss: 0.93351  val_acc: 0.78227  elapsed: 18m 16s\n================ ༼ つ ◕_◕ ༽つ Epoch 10 - train_loss: 0.13300  val_loss: 0.97920  val_acc: 0.78227  elapsed: 20m 17s\n================ ༼ つ ◕_◕ ༽つ Epoch 11 - train_loss: 0.08469  val_loss: 0.95283  val_acc: 0.78227  elapsed: 22m 17s\n================ ༼ つ ◕_◕ ༽つ Epoch 12 - train_loss: 0.09832  val_loss: 1.12334  val_acc: 0.78227  elapsed: 24m 18s\n================ ༼ つ ◕_◕ ༽つ Epoch 13 - train_loss: 0.10285  val_loss: 1.11590  val_acc: 0.78227  elapsed: 26m 19s\n================ ༼ つ ◕_◕ ༽つ Epoch 14 - train_loss: 0.08521  val_loss: 1.00062  val_acc: 0.78227  elapsed: 28m 20s\n================ ༼ つ ◕_◕ ༽つ Epoch 15 - train_loss: 0.06842  val_loss: 0.99412  val_acc: 0.78227  elapsed: 30m 20s\n================ ༼ つ ◕_◕ ༽つ Epoch 16 - train_loss: 0.08416  val_loss: 1.00664  val_acc: 0.78227  elapsed: 32m 21s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 17 - train_loss: 0.10386  val_loss: 0.96765  val_acc: 0.78420  elapsed: 34m 22s\n================ ༼ つ ◕_◕ ༽つ Epoch 18 - train_loss: 0.09676  val_loss: 1.00480  val_acc: 0.78420  elapsed: 36m 25s\n================ ༼ つ ◕_◕ ༽つ Epoch 19 - train_loss: 0.07120  val_loss: 1.10077  val_acc: 0.78420  elapsed: 38m 26s\n================ ༼ つ ◕_◕ ༽つ Epoch 20 - train_loss: 0.06430  val_loss: 1.13731  val_acc: 0.78420  elapsed: 40m 26s\n============== ༼ つ ◕_◕ ༽つ BEST epoch : 20, Accuracy : 0.7842003853564548 ====================================\n========================== ༼ つ ◕_◕ ༽つ Model Load 0_th FOLD =================================\npredict values check :  [ 0.14842749 -2.24459887 -1.41422296 -2.43978524  5.56320858]\n2/5 fold training starts!\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 1 - train_loss: 1.32159  val_loss: 1.15439  val_acc: 0.54335  elapsed: 42m 37s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 2 - train_loss: 0.98286  val_loss: 0.75945  val_acc: 0.73218  elapsed: 44m 38s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 3 - train_loss: 0.70222  val_loss: 0.68672  val_acc: 0.74952  elapsed: 46m 42s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 4 - train_loss: 0.54980  val_loss: 0.59954  val_acc: 0.78998  elapsed: 48m 46s\n================ ༼ つ ◕_◕ ༽つ Epoch 5 - train_loss: 0.40589  val_loss: 0.71310  val_acc: 0.78998  elapsed: 50m 51s\n================ ༼ つ ◕_◕ ༽つ Epoch 6 - train_loss: 0.34540  val_loss: 0.72927  val_acc: 0.78998  elapsed: 52m 51s\n================ ༼ つ ◕_◕ ༽つ Epoch 7 - train_loss: 0.26432  val_loss: 0.80405  val_acc: 0.78998  elapsed: 54m 52s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 8 - train_loss: 0.24469  val_loss: 0.75764  val_acc: 0.79769  elapsed: 56m 53s\n================ ༼ つ ◕_◕ ༽つ Epoch 9 - train_loss: 0.21626  val_loss: 0.64837  val_acc: 0.79769  elapsed: 58m 57s\n================ ༼ つ ◕_◕ ༽つ Epoch 10 - train_loss: 0.18238  val_loss: 0.78891  val_acc: 0.79769  elapsed: 60m 58s\n================ ༼ つ ◕_◕ ༽つ Epoch 11 - train_loss: 0.19780  val_loss: 0.86950  val_acc: 0.79769  elapsed: 62m 59s\n================ ༼ つ ◕_◕ ༽つ Epoch 12 - train_loss: 0.15875  val_loss: 0.81680  val_acc: 0.79769  elapsed: 64m 59s\n================ ༼ つ ◕_◕ ༽つ Epoch 13 - train_loss: 0.14747  val_loss: 0.76136  val_acc: 0.79769  elapsed: 67m 0s\n================ ༼ つ ◕_◕ ༽つ Epoch 14 - train_loss: 0.13357  val_loss: 0.89003  val_acc: 0.79769  elapsed: 69m 1s\n================ ༼ つ ◕_◕ ༽つ Epoch 15 - train_loss: 0.14930  val_loss: 0.95050  val_acc: 0.79769  elapsed: 71m 1s\n================ ༼ つ ◕_◕ ༽つ Epoch 16 - train_loss: 0.15865  val_loss: 0.92523  val_acc: 0.79769  elapsed: 73m 2s\n================ ༼ つ ◕_◕ ༽つ Epoch 17 - train_loss: 0.14740  val_loss: 0.80140  val_acc: 0.79769  elapsed: 75m 3s\n================ ༼ つ ◕_◕ ༽つ Epoch 18 - train_loss: 0.13094  val_loss: 0.89591  val_acc: 0.79769  elapsed: 77m 4s\n================ ༼ つ ◕_◕ ༽つ Epoch 19 - train_loss: 0.12310  val_loss: 0.92397  val_acc: 0.79769  elapsed: 79m 5s\n================ ༼ つ ◕_◕ ༽つ Epoch 20 - train_loss: 0.09809  val_loss: 0.95403  val_acc: 0.79769  elapsed: 81m 5s\n============== ༼ つ ◕_◕ ༽つ BEST epoch : 20, Accuracy : 0.7976878612716763 ====================================\n========================== ༼ つ ◕_◕ ༽つ Model Load 1_th FOLD =================================\npredict values check :  [-0.92586726 -1.99929249 -1.06066322 -1.46056604  6.00079679]\n3/5 fold training starts!\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 1 - train_loss: 1.49336  val_loss: 1.25281  val_acc: 0.55019  elapsed: 83m 16s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 2 - train_loss: 1.13364  val_loss: 0.95107  val_acc: 0.65637  elapsed: 85m 16s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 3 - train_loss: 0.78998  val_loss: 0.86761  val_acc: 0.69305  elapsed: 87m 20s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 4 - train_loss: 0.56138  val_loss: 0.77332  val_acc: 0.76641  elapsed: 89m 24s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 5 - train_loss: 0.43897  val_loss: 0.66069  val_acc: 0.78571  elapsed: 91m 27s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 6 - train_loss: 0.36837  val_loss: 0.73687  val_acc: 0.78958  elapsed: 93m 30s\n================ ༼ つ ◕_◕ ༽つ Epoch 7 - train_loss: 0.27564  val_loss: 0.78324  val_acc: 0.78958  elapsed: 95m 33s\n================ ༼ つ ◕_◕ ༽つ Epoch 8 - train_loss: 0.21723  val_loss: 0.79102  val_acc: 0.78958  elapsed: 97m 34s\n================ ༼ つ ◕_◕ ༽つ Epoch 9 - train_loss: 0.21209  val_loss: 0.88459  val_acc: 0.78958  elapsed: 99m 35s\n================ ༼ つ ◕_◕ ༽つ Epoch 10 - train_loss: 0.16691  val_loss: 0.90861  val_acc: 0.78958  elapsed: 101m 36s\n================ ༼ つ ◕_◕ ༽つ Epoch 11 - train_loss: 0.17538  val_loss: 0.85940  val_acc: 0.78958  elapsed: 103m 37s\n================ ༼ つ ◕_◕ ༽つ Epoch 12 - train_loss: 0.13942  val_loss: 0.91572  val_acc: 0.78958  elapsed: 105m 38s\n================ ༼ つ ◕_◕ ༽つ Epoch 13 - train_loss: 0.11889  val_loss: 0.96795  val_acc: 0.78958  elapsed: 107m 38s\n================ ༼ つ ◕_◕ ༽つ Epoch 14 - train_loss: 0.13371  val_loss: 0.95303  val_acc: 0.78958  elapsed: 109m 39s\n================ ༼ つ ◕_◕ ༽つ Epoch 15 - train_loss: 0.12722  val_loss: 1.02936  val_acc: 0.78958  elapsed: 111m 40s\n================ ༼ つ ◕_◕ ༽つ Epoch 16 - train_loss: 0.13457  val_loss: 0.97776  val_acc: 0.78958  elapsed: 113m 41s\n================ ༼ つ ◕_◕ ༽つ Epoch 17 - train_loss: 0.13513  val_loss: 0.91336  val_acc: 0.78958  elapsed: 115m 42s\n================ ༼ つ ◕_◕ ༽つ Epoch 18 - train_loss: 0.08803  val_loss: 1.04006  val_acc: 0.78958  elapsed: 117m 43s\n================ ༼ つ ◕_◕ ༽つ Epoch 19 - train_loss: 0.11627  val_loss: 0.93802  val_acc: 0.78958  elapsed: 119m 44s\n================ ༼ つ ◕_◕ ༽つ Epoch 20 - train_loss: 0.09937  val_loss: 1.17975  val_acc: 0.78958  elapsed: 121m 45s\n============== ༼ つ ◕_◕ ༽つ BEST epoch : 20, Accuracy : 0.7895752895752896 ====================================\n========================== ༼ つ ◕_◕ ༽つ Model Load 2_th FOLD =================================\npredict values check :  [-1.38709331 -1.81836426 -1.15071213 -0.63080519  4.95069838]\n4/5 fold training starts!\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 1 - train_loss: 1.20192  val_loss: 0.86821  val_acc: 0.68340  elapsed: 123m 55s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 2 - train_loss: 0.73193  val_loss: 0.64144  val_acc: 0.78571  elapsed: 125m 56s\n================ ༼ つ ◕_◕ ༽つ Epoch 3 - train_loss: 0.50993  val_loss: 0.70621  val_acc: 0.78571  elapsed: 127m 59s\n================ ༼ つ ◕_◕ ༽つ Epoch 4 - train_loss: 0.38910  val_loss: 0.64897  val_acc: 0.78571  elapsed: 129m 60s\n================ ༼ つ ◕_◕ ༽つ Epoch 5 - train_loss: 0.25845  val_loss: 0.79538  val_acc: 0.78571  elapsed: 132m 1s\n================ ༼ つ ◕_◕ ༽つ Epoch 6 - train_loss: 0.23407  val_loss: 0.82619  val_acc: 0.78571  elapsed: 134m 2s\n================ ༼ つ ◕_◕ ༽つ Epoch 7 - train_loss: 0.16959  val_loss: 0.83436  val_acc: 0.78571  elapsed: 136m 3s\n================ ༼ つ ◕_◕ ༽つ Epoch 8 - train_loss: 0.15745  val_loss: 0.98849  val_acc: 0.78571  elapsed: 138m 3s\n================ ༼ つ ◕_◕ ༽つ Epoch 9 - train_loss: 0.11469  val_loss: 0.85840  val_acc: 0.78571  elapsed: 140m 4s\n================ ༼ つ ◕_◕ ༽つ Epoch 10 - train_loss: 0.13926  val_loss: 0.90893  val_acc: 0.78571  elapsed: 142m 5s\n================ ༼ つ ◕_◕ ༽つ Epoch 11 - train_loss: 0.12136  val_loss: 0.92402  val_acc: 0.78571  elapsed: 144m 6s\n================ ༼ つ ◕_◕ ༽つ Epoch 12 - train_loss: 0.10765  val_loss: 1.14621  val_acc: 0.78571  elapsed: 146m 7s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 13 - train_loss: 0.11918  val_loss: 0.88648  val_acc: 0.78958  elapsed: 148m 7s\n================ ༼ つ ◕_◕ ༽つ Epoch 14 - train_loss: 0.09416  val_loss: 0.99207  val_acc: 0.78958  elapsed: 150m 11s\n================ ༼ つ ◕_◕ ༽つ Epoch 15 - train_loss: 0.11350  val_loss: 1.00237  val_acc: 0.78958  elapsed: 152m 12s\n================ ༼ つ ◕_◕ ༽つ Epoch 16 - train_loss: 0.08602  val_loss: 0.96990  val_acc: 0.78958  elapsed: 154m 12s\n================ ༼ つ ◕_◕ ༽つ Epoch 17 - train_loss: 0.09296  val_loss: 1.05263  val_acc: 0.78958  elapsed: 156m 13s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 18 - train_loss: 0.06698  val_loss: 1.04607  val_acc: 0.79151  elapsed: 158m 14s\n================ ༼ つ ◕_◕ ༽つ Epoch 19 - train_loss: 0.12295  val_loss: 1.04926  val_acc: 0.79151  elapsed: 160m 18s\n================ ༼ つ ◕_◕ ༽つ Epoch 20 - train_loss: 0.09237  val_loss: 1.07598  val_acc: 0.79151  elapsed: 162m 19s\n============== ༼ つ ◕_◕ ༽つ BEST epoch : 20, Accuracy : 0.7915057915057915 ====================================\n========================== ༼ つ ◕_◕ ༽つ Model Load 3_th FOLD =================================\npredict values check :  [-0.11947107 -1.91525674 -1.45128298 -2.76274061  6.3028717 ]\n5/5 fold training starts!\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 1 - train_loss: 1.25260  val_loss: 1.02541  val_acc: 0.63320  elapsed: 164m 29s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 2 - train_loss: 0.81605  val_loss: 0.79240  val_acc: 0.72394  elapsed: 166m 30s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 3 - train_loss: 0.58381  val_loss: 0.78180  val_acc: 0.74131  elapsed: 168m 34s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 4 - train_loss: 0.44209  val_loss: 0.73435  val_acc: 0.75869  elapsed: 170m 38s\n================ ༼ つ ◕_◕ ༽つ Epoch 5 - train_loss: 0.33352  val_loss: 0.88780  val_acc: 0.75869  elapsed: 172m 41s\n================ ༼ つ ◕_◕ ༽つ Epoch 6 - train_loss: 0.26672  val_loss: 1.06083  val_acc: 0.75869  elapsed: 174m 42s\n================ ༼ つ ◕_◕ ༽つ Epoch 7 - train_loss: 0.20232  val_loss: 0.97419  val_acc: 0.75869  elapsed: 176m 42s\n================ ༼ つ ◕_◕ ༽つ Epoch 8 - train_loss: 0.16839  val_loss: 1.15174  val_acc: 0.75869  elapsed: 178m 42s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 9 - train_loss: 0.13590  val_loss: 1.04029  val_acc: 0.76448  elapsed: 180m 42s\n================ ༼ つ ◕_◕ ༽つ Epoch 10 - train_loss: 0.09963  val_loss: 1.09606  val_acc: 0.76448  elapsed: 182m 46s\n================ ༼ つ ◕_◕ ༽つ Epoch 11 - train_loss: 0.10456  val_loss: 1.13734  val_acc: 0.76448  elapsed: 184m 47s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 12 - train_loss: 0.13846  val_loss: 1.08006  val_acc: 0.76641  elapsed: 186m 47s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 13 - train_loss: 0.11463  val_loss: 1.19392  val_acc: 0.76834  elapsed: 188m 50s\n================ ༼ つ ◕_◕ ༽つ Epoch 14 - train_loss: 0.10251  val_loss: 0.95286  val_acc: 0.76834  elapsed: 190m 53s\n================ ༼ つ ◕_◕ ༽つ Epoch 15 - train_loss: 0.08830  val_loss: 1.14709  val_acc: 0.76834  elapsed: 192m 54s\n================ ༼ つ ◕_◕ ༽つ Epoch 16 - train_loss: 0.07514  val_loss: 1.34402  val_acc: 0.76834  elapsed: 194m 54s\n================ ༼ つ ◕_◕ ༽つ Epoch 17 - train_loss: 0.09495  val_loss: 1.29304  val_acc: 0.76834  elapsed: 196m 54s\n================ ༼ つ ◕_◕ ༽つ Epoch 18 - train_loss: 0.09898  val_loss: 1.06983  val_acc: 0.76834  elapsed: 198m 55s\n================ ༼ つ ◕_◕ ༽つ Epoch 19 - train_loss: 0.08485  val_loss: 1.22385  val_acc: 0.76834  elapsed: 200m 55s\n================ ༼ つ ◕_◕ ༽つ Epoch 20 - train_loss: 0.08991  val_loss: 1.15294  val_acc: 0.76834  elapsed: 202m 56s\n============== ༼ つ ◕_◕ ༽つ BEST epoch : 20, Accuracy : 0.7683397683397684 ====================================\n========================== ༼ つ ◕_◕ ༽つ Model Load 4_th FOLD =================================\npredict values check :  [-1.93310392 -1.74500418 -0.51802552 -1.22778761  6.07836246]\nCPU times: user 3h 14min 54s, sys: 7min 30s, total: 3h 22min 24s\nWall time: 3h 23min 3s\n"
    }
   ],
   "source": [
    "%%time\n",
    "best_epoch_list = []\n",
    "best_val_acc_list = []\n",
    "start_time = time()\n",
    "\n",
    "splits = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED).split(x_train, y_train))\n",
    "for fold in [0, 1, 2, 3, 4]:\n",
    "\n",
    "    print(\"============= ༼ つ ◕_◕ ༽つ {}/5 fold training starts ! =============\".format(fold+1))\n",
    "    \n",
    "    fold_num = str(fold + 1)\n",
    "\n",
    "    trn_index, val_index = splits[fold]\n",
    "\n",
    "    X_train, X_valid = x_train[trn_index], x_train[val_index]\n",
    "    #train_length, valid_length = lengths[trn_index], lengths[val_index]\n",
    "    Y_train, Y_valid = y_train[trn_index], y_train[val_index]\n",
    "\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype = torch.long), torch.tensor(Y_train, dtype=torch.long)) #TensorDataset(X_train, train_length, torch.tensor(Y_train))\n",
    "    valid_dataset = TensorDataset(torch.tensor(X_valid, dtype = torch.long), torch.tensor(Y_valid, dtype=torch.long)) #TensorDataset(X_valid, valid_length, torch.tensor(Y_valid))\n",
    "    \n",
    "    model = BertForSequenceClassification.from_pretrained(\"../job_nlp/working\",cache_dir=None, num_labels=5)\n",
    "    model.zero_grad()\n",
    "    model = model.to(device)\n",
    "    #optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "    #                 lr=lr,\n",
    "    #                 warmup=0.05,\n",
    "    #                 t_total=num_train_optimization_steps)\n",
    "    #scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    \n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "    #train = train_dataset\n",
    "\n",
    "    num_train_optimization_steps = int(EPOCHS*len(train_dataset)/batch_size/accumulation_steps)\n",
    "\n",
    "    #optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "    #                     lr=lr,\n",
    "    #                     warmup=0.05,\n",
    "    #                     t_total=np.ceil(num_train_optimization_steps))\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr, weight_decay=0.000025)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "    best_valid_score = 0\n",
    "    best_val_acc = 0\n",
    "    \n",
    "    #tq = tqdm_notebook(range(EPOCHS))\n",
    "    #model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\",verbosity=0)\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        \n",
    "        #start_time = time.time()\n",
    "        train_loss = 0\n",
    "        train_total_correct = 0\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        #tk0 = tqdm_notebook(enumerate(train_loader),total=len(train_loader),leave=False)\n",
    "        \n",
    "        for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            preds  = model(x_batch.to(device), attention_mask = (x_batch>0).to(device), labels=None)\n",
    "            loss = criterion(preds, y_batch.to(device))\n",
    "            loss.backward()\n",
    "            \n",
    "            if (i+1) % accumulation_steps == 0:             # Wait for several backward steps\n",
    "                optimizer.step()                            # Now we can do an optimizer step\n",
    "                optimizer.zero_grad()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            train_loss += loss.item()/len(train_loader)\n",
    "            \n",
    "        # Validation Starts\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        valid_total_correct = 0\n",
    "        \n",
    "        #valid_preds = np.zeros(len(valid_dataset),5)\n",
    "        #valid_targets = np.zeros(len(valid_dataset),5)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "                #valid_targets[i*batch_size: (i+1)*batch_size] = y_batch.numpy().copy()\n",
    "                \n",
    "                preds = model(x_batch.to(device), attention_mask = (x_batch>0).to(device), labels=None)\n",
    "                \n",
    "                loss = criterion(preds, y_batch.to(device))\n",
    "                \n",
    "                output_prob = F.softmax(preds, dim=1)\n",
    "\n",
    "                predict_vector = np.argmax(to_numpy(output_prob), axis=1)\n",
    "                label_vector = to_numpy(y_batch)\n",
    "                #valid_preds[i*batch_size: (i+1)*batch_size] = np.argmax(preds_prob.detach().cpu().squeeze().numpy())\n",
    "                bool_vector = predict_vector == label_vector\n",
    "                \n",
    "                val_loss += loss.item()/len(valid_loader)\n",
    "                valid_total_correct += bool_vector.sum()\n",
    "        \n",
    "        #val_score = roc_auc_score(valid_targets, valid_preds)\n",
    "\n",
    "        elapsed = time() - start_time\n",
    "        val_acc = valid_total_correct / len(valid_loader.dataset)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch\n",
    "            print(\"val_acc has improved !! \")\n",
    "            best_epoch_list.append(best_epoch)\n",
    "            best_val_acc_list.append(best_val_acc)\n",
    "            \n",
    "            torch.save(model.state_dict(), '../job_nlp/Bert_20e_maxseq600_fold_{}.pt'.format(fold))\n",
    "            #print(\"================ ༼ つ ◕_◕ ༽つ BEST epoch : {}, Accuracy : {} \".format(epoch, best_val_acc))\n",
    "            \n",
    "        #lr = [_['lr'] for _ in optimizer.param_g] # or optimizer\n",
    "        print(\"================ ༼ つ ◕_◕ ༽つ Epoch {} - train_loss: {:.5f}  val_loss: {:.5f}  val_acc: {:.5f}  elapsed: {:.0f}m {:.0f}s\".format(epoch, train_loss, val_loss, best_val_acc, elapsed // 60, elapsed % 60))\n",
    "    print(\"============== ༼ つ ◕_◕ ༽つ BEST epoch : {}, Accuracy : {} ====================================\".format(epoch, best_val_acc))\n",
    "    #best_epoch_list.append(best_epoch)\n",
    "    #best_val_acc_list.append(best_val_acc)\n",
    "    \n",
    "    #---- Inference ----\n",
    "    #batch_size = 8\n",
    "\n",
    "    print(\"========================== ༼ つ ◕_◕ ༽つ Model Load {}_th FOLD =================================\".format(fold))\n",
    "    model.load_state_dict(torch.load('Bert_20e_maxseq600_fold_{}.pt'.format(fold)))\n",
    "    model.eval()\n",
    "    predictions = np.zeros((len(test_loader.dataset),5))\n",
    "    with torch.no_grad():\n",
    "        for i, (x_batch, ) in enumerate(test_loader):\n",
    "            preds = model(x_batch.to(device), attention_mask = (x_batch>0).to(device), labels=None)\n",
    "            \n",
    "            predictions[i*batch_size: (i+1)*batch_size] = to_numpy(preds)\n",
    "    print(\"predict values check : \",predictions[0])\n",
    "    np.savetxt(\"../job_nlp/bert_raw_submission/bert_20e_maxseq600_fold_{}.csv\".format(fold), predictions, delimiter=\",\")   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}