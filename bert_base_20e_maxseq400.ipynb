{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from time import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "import datetime\n",
    "import pkg_resources\n",
    "#import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import gc\n",
    "import re\n",
    "import operator \n",
    "import sys\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, Subset, DataLoader\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "#from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "#%matplotlib inline\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import os\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "import pickle\n",
    "#from apex import amp\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=123):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class AdamW(Optimizer):\n",
    "    \"\"\"Implements AdamW algorithm.\n",
    "\n",
    "    It has been proposed in `Fixing Weight Decay Regularization in Adam`_.\n",
    "\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float, optional): learning rate (default: 1e-3)\n",
    "        betas (Tuple[float, float], optional): coefficients used for computing\n",
    "            running averages of gradient and its square (default: (0.9, 0.999))\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability (default: 1e-8)\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "\n",
    "    .. Fixing Weight Decay Regularization in Adam:\n",
    "    https://arxiv.org/abs/1711.05101\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay)\n",
    "        super(AdamW, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('AdamW does not support sparse gradients, please consider SparseAdam instead')\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                # according to the paper, this penalty should come after the bias correction\n",
    "                # if group['weight_decay'] != 0:\n",
    "                #     grad = grad.add(group['weight_decay'], p.data)\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "\n",
    "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p.data.add_(-group['weight_decay'], p.data)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 295 \n",
    "SEED = 42\n",
    "EPOCHS = 20\n",
    "Data_dir=\"../job_nlp/\"\n",
    "WORK_DIR = \"../job_nlp/working/\"\n",
    "#num_to_load=100000                         #Train size to match time limit\n",
    "#valid_size= 50000                          #Validation Size\n",
    "TARGET = 'smishing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/matsuik/ppbert\n",
    "package_dir_a = \"../job_nlp/ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT\"\n",
    "sys.path.insert(0, package_dir_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/home/yilgukseo/anaconda3/envs/pytorch/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n  return f(*args, **kwds)\n/home/yilgukseo/anaconda3/envs/pytorch/lib/python3.7/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n  return f(*args, **kwds)\n/home/yilgukseo/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py:5879: ResourceWarning: unclosed file <_io.TextIOWrapper name='/home/yilgukseo/.keras/keras.json' mode='r' encoding='UTF-8'>\n  _config = json.load(open(_config_path))\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
    }
   ],
   "source": [
    "from pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification,BertAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "erNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_9/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_9/output/dense/kernel with shape [3072, 768]\nLoading TF weight bert/pooler/dense/bias with shape [768]\nLoading TF weight bert/pooler/dense/kernel with shape [768, 768]\nLoading TF weight cls/predictions/output_bias with shape [30522]\nLoading TF weight cls/predictions/transform/LayerNorm/beta with shape [768]\nLoading TF weight cls/predictions/transform/LayerNorm/gamma with shape [768]\nLoading TF weight cls/predictions/transform/dense/bias with shape [768]\nLoading TF weight cls/predictions/transform/dense/kernel with shape [768, 768]\nLoading TF weight cls/seq_relationship/output_bias with shape [2]\nLoading TF weight cls/seq_relationship/output_weights with shape [2, 768]\nInitialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\nInitialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\nInitialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\nInitialize PyTorch weight ['cls', 'predictions', 'output_bias']\nInitialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'bias']\nInitialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'kernel']\nInitialize PyTorch weight ['cls', 'seq_relationship', 'output_bias']\nInitialize PyTorch weight ['cls', 'seq_relationship', 'output_weights']\nSave PyTorch model to ../job_nlp/working/pytorch_model.bin\n"
    },
    {
     "data": {
      "text/plain": "'../job_nlp/working/bert_config.json'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Translate model from tensorflow to pytorch\n",
    "BERT_MODEL_PATH = '../job_nlp/bert-pretrained-models/uncased_L-12_H-768_A-12/uncased_L-12_H-768_A-12/'\n",
    "convert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n",
    "    BERT_MODEL_PATH + 'bert_model.ckpt',\n",
    "BERT_MODEL_PATH + 'bert_config.json',\n",
    "WORK_DIR + 'pytorch_model.bin')\n",
    "\n",
    "shutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', WORK_DIR + 'bert_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the Bert configuration file\n",
    "from pytorch_pretrained_bert import BertConfig\n",
    "\n",
    "bert_config = BertConfig('../job_nlp/bert-pretrained-models/uncased_L-12_H-768_A-12/uncased_L-12_H-768_A-12/'+'bert_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"max_position_embeddings\": 512,\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 30522\n}"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the lines to BERT format\n",
    "# Thanks to https://www.kaggle.com/httpwwwfszyc/bert-in-keras-taming\n",
    "def convert_lines(example, max_seq_length,tokenizer):\n",
    "    max_seq_length -=2\n",
    "    all_tokens = []\n",
    "    longer = 0\n",
    "    for text in tqdm_notebook(example):\n",
    "        tokens_a = tokenizer.tokenize(text)\n",
    "        if len(tokens_a)>max_seq_length:\n",
    "            tokens_a = tokens_a[:max_seq_length]\n",
    "            longer += 1\n",
    "        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n",
    "        all_tokens.append(one_token)\n",
    "    print(longer)\n",
    "    return np.array(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_PATH = '../job_nlp/bert-pretrained-models/uncased_L-12_H-768_A-12/uncased_L-12_H-768_A-12/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 31.9 ms, sys: 0 ns, total: 31.9 ms\nWall time: 31.7 ms\n"
    }
   ],
   "source": [
    "%%time\n",
    "train_df = pd.read_csv(os.path.join(Data_dir,\"train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(Data_dir,\"test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "3    0.354552\n5    0.248457\n4    0.184414\n2    0.113812\n1    0.098765\nName: label, dtype: float64"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "title      [$$$] 재귀함수를 처리하면서 setTimeout으로 딜레이를 주면 읽는 순서가 ...\ncontent                                                  NaN\nlabel                                                      4\nName: 2400, dtype: object\n"
    }
   ],
   "source": [
    "# replace NaN\n",
    "print(train_df.iloc[2400,:])\n",
    "train_df.iloc[2400,1] = \"@@@\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 39.5 ms, sys: 0 ns, total: 39.5 ms\nWall time: 39.3 ms\n"
    }
   ],
   "source": [
    "%%time\n",
    "train_df['text'] = train_df[['title', 'content']].apply(lambda x: ' '.join(x), axis = 1)\n",
    "test_df['text'] = test_df[['title', 'content']].apply(lambda x: ' '.join(x), axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>content</th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>wait()과 sleep()의 차이점은 뭔가요</td>\n      <td>###발생하는 문제 및 실행환경\\nwait()과 sleep()의 차이점은 뭔가요</td>\n      <td>3</td>\n      <td>wait()과 sleep()의 차이점은 뭔가요 ###발생하는 문제 및 실행환경\\nw...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>$$$ 초보 외부 클래스 멤버 변수 사용 질문합니다.ㅠㅠ</td>\n      <td>1.헤더에, 사용할 멤버변수가 담긴 헤더 파일이 Include 되어있습니다.\\n예를...</td>\n      <td>2</td>\n      <td>$$$ 초보 외부 클래스 멤버 변수 사용 질문합니다.ㅠㅠ 1.헤더에, 사용할 멤버변...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ORM: Sequelize: 다대다 관계 쿼리</td>\n      <td>안녕하세요.\\n\\n어떻게 다대다 관계 쿼리를 해야하나요? 예를들어, `product...</td>\n      <td>4</td>\n      <td>ORM: Sequelize: 다대다 관계 쿼리 안녕하세요.\\n\\n어떻게 다대다 관계...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>$$$에서 숫자가 NaN인지 검사하려면 어떻게해야하죠?</td>\n      <td>```\\nparseFloat('geoff') == NaN;\\n\\nparseFloat...</td>\n      <td>4</td>\n      <td>$$$에서 숫자가 NaN인지 검사하려면 어떻게해야하죠? ```\\nparseFloat...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>$$$ 2.7에서 중국어, 특수문자 인코딩 하는 방법이 뭔가요?</td>\n      <td>```\\n&gt;&gt;&gt;dict['name']\\n胡安·马塔\\n&gt;&gt;&gt;json.dumps(dic...</td>\n      <td>5</td>\n      <td>$$$ 2.7에서 중국어, 특수문자 인코딩 하는 방법이 뭔가요? ```\\n&gt;&gt;&gt;di...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                 title  \\\n0            wait()과 sleep()의 차이점은 뭔가요   \n1      $$$ 초보 외부 클래스 멤버 변수 사용 질문합니다.ㅠㅠ   \n2            ORM: Sequelize: 다대다 관계 쿼리   \n3       $$$에서 숫자가 NaN인지 검사하려면 어떻게해야하죠?   \n4  $$$ 2.7에서 중국어, 특수문자 인코딩 하는 방법이 뭔가요?   \n\n                                             content  label  \\\n0       ###발생하는 문제 및 실행환경\\nwait()과 sleep()의 차이점은 뭔가요      3   \n1  1.헤더에, 사용할 멤버변수가 담긴 헤더 파일이 Include 되어있습니다.\\n예를...      2   \n2  안녕하세요.\\n\\n어떻게 다대다 관계 쿼리를 해야하나요? 예를들어, `product...      4   \n3  ```\\nparseFloat('geoff') == NaN;\\n\\nparseFloat...      4   \n4  ```\\n>>>dict['name']\\n胡安·马塔\\n>>>json.dumps(dic...      5   \n\n                                                text  \n0  wait()과 sleep()의 차이점은 뭔가요 ###발생하는 문제 및 실행환경\\nw...  \n1  $$$ 초보 외부 클래스 멤버 변수 사용 질문합니다.ㅠㅠ 1.헤더에, 사용할 멤버변...  \n2  ORM: Sequelize: 다대다 관계 쿼리 안녕하세요.\\n\\n어떻게 다대다 관계...  \n3  $$$에서 숫자가 NaN인지 검사하려면 어떻게해야하죠? ```\\nparseFloat...  \n4  $$$ 2.7에서 중국어, 특수문자 인코딩 하는 방법이 뭔가요? ```\\n>>>di...  "
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "25"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.iloc[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "43"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.iloc[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "69"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.iloc[0,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['t_length'] = train_df['title'].apply(lambda x: len(x))\n",
    "train_df['c_length'] = train_df['content'].apply(lambda x: len(x))\n",
    "train_df['text_length'] = train_df['text'].apply(lambda x: len(x))\n",
    "\n",
    "test_df['t_length'] = test_df['title'].apply(lambda x: len(x))\n",
    "test_df['c_length'] = test_df['content'].apply(lambda x: len(x))\n",
    "test_df['text_length'] = test_df['text'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>t_length</th>\n      <th>c_length</th>\n      <th>text_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>2592.000000</td>\n      <td>2592.000000</td>\n      <td>2592.000000</td>\n      <td>2592.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>3.369985</td>\n      <td>29.945988</td>\n      <td>891.706404</td>\n      <td>922.652392</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.245321</td>\n      <td>15.235909</td>\n      <td>1684.607332</td>\n      <td>1685.667323</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>3.000000</td>\n      <td>8.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>3.000000</td>\n      <td>19.000000</td>\n      <td>180.000000</td>\n      <td>208.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>3.000000</td>\n      <td>27.000000</td>\n      <td>385.000000</td>\n      <td>418.500000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>4.000000</td>\n      <td>37.000000</td>\n      <td>896.000000</td>\n      <td>920.750000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>5.000000</td>\n      <td>119.000000</td>\n      <td>28756.000000</td>\n      <td>28780.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "             label     t_length      c_length   text_length\ncount  2592.000000  2592.000000   2592.000000   2592.000000\nmean      3.369985    29.945988    891.706404    922.652392\nstd       1.245321    15.235909   1684.607332   1685.667323\nmin       1.000000     3.000000      3.000000      8.000000\n25%       3.000000    19.000000    180.000000    208.000000\n50%       3.000000    27.000000    385.000000    418.500000\n75%       4.000000    37.000000    896.000000    920.750000\nmax       5.000000   119.000000  28756.000000  28780.000000"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>t_length</th>\n      <th>c_length</th>\n      <th>text_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>500.000000</td>\n      <td>500.000000</td>\n      <td>500.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>29.310000</td>\n      <td>858.270000</td>\n      <td>888.580000</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>14.786046</td>\n      <td>1957.436752</td>\n      <td>1956.747171</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>5.000000</td>\n      <td>20.000000</td>\n      <td>53.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>19.750000</td>\n      <td>192.250000</td>\n      <td>227.250000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>26.500000</td>\n      <td>371.000000</td>\n      <td>399.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>36.000000</td>\n      <td>841.250000</td>\n      <td>870.500000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>134.000000</td>\n      <td>35210.000000</td>\n      <td>35226.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "         t_length      c_length   text_length\ncount  500.000000    500.000000    500.000000\nmean    29.310000    858.270000    888.580000\nstd     14.786046   1957.436752   1956.747171\nmin      5.000000     20.000000     53.000000\n25%     19.750000    192.250000    227.250000\n50%     26.500000    371.000000    399.000000\n75%     36.000000    841.250000    870.500000\nmax    134.000000  35210.000000  35226.000000"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "loaded 2592 records\n/home/yilgukseo/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  import sys\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6fc938de22a4ef29ea35bcd3180f0d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=2592.0), HTML(value='')))"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n1065\nX_train : 2592\nloaded 500 records\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bed23cce1d944c2bae7e78de61f3a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=500.0), HTML(value='')))"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n208\nX_test : 500\n"
    }
   ],
   "source": [
    "#%%time\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)\n",
    "#train_df = pd.read_csv(os.path.join(Data_dir,\"train.csv\"))#.sample(num_to_load+valid_size,random_state=SEED)\n",
    "print('loaded %d records' % len(train_df))\n",
    "# Make sure all comment_text values are strings\n",
    "train_df['text'] = train_df['text'].astype(str) \n",
    "x_train = convert_lines(train_df[\"text\"].fillna(\"DUMMY_VALUE\"),MAX_SEQUENCE_LENGTH,tokenizer)\n",
    "print(\"X_train : {}\".format(len(x_train)))\n",
    "\n",
    "#test_df = pd.read_csv(os.path.join(Data_dir,\"public_test.csv\"))#.sample(num_to_load+valid_size,random_state=SEED)\n",
    "print('loaded %d records' % len(test_df))\n",
    "test_df['text'] = test_df['text'].astype(str) \n",
    "x_test = convert_lines(test_df[\"text\"].fillna(\"DUMMY_VALUE\"),MAX_SEQUENCE_LENGTH,tokenizer)\n",
    "print(\"X_test : {}\".format(len(x_test)))\n",
    "\n",
    "train_df=train_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# above not working in linux ?? these x_train & x_test are obtained from windows\n",
    "#x_train = np.loadtxt('../job_nlp/x_train.csv', delimiter=',')\n",
    "#x_test = np.loadtxt('../job_nlp/x_test.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(SEED)\n",
    "output_model_file = \"bert_pytorch.bin\"\n",
    "\n",
    "lr=2e-5\n",
    "batch_size = 8\n",
    "accumulation_steps=2\n",
    "n_labels = 2\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "TARGET = 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[TARGET] = train_df[TARGET]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([2, 1, 3,  ..., 0, 4, 1])"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x_train = train_df['text']\n",
    "y_train = torch.tensor(train_df[TARGET])#.long()\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([2, 1, 3, 3, 4])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(x):\n",
    "    return x.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(torch.tensor(x_test, dtype = torch.long)) #TensorDataset(X_valid, valid_length, torch.tensor(Y_valid))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"../job_nlp/working\",cache_dir=None, num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "1/5 fold training starts!\n/home/yilgukseo/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 1 - train_loss: 1.30095  val_loss: 1.10379  val_acc: 0.56069  elapsed: 1m 29s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 2 - train_loss: 1.01166  val_loss: 0.84731  val_acc: 0.69557  elapsed: 2m 55s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 3 - train_loss: 0.70330  val_loss: 0.69070  val_acc: 0.75915  elapsed: 4m 24s\n================ ༼ つ ◕_◕ ༽つ Epoch 4 - train_loss: 0.52543  val_loss: 0.69923  val_acc: 0.75915  elapsed: 5m 54s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 5 - train_loss: 0.39848  val_loss: 0.64605  val_acc: 0.76686  elapsed: 7m 20s\n================ ༼ つ ◕_◕ ༽つ Epoch 6 - train_loss: 0.27032  val_loss: 0.85986  val_acc: 0.76686  elapsed: 8m 49s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 7 - train_loss: 0.22332  val_loss: 0.77685  val_acc: 0.78035  elapsed: 10m 15s\n================ ༼ つ ◕_◕ ༽つ Epoch 8 - train_loss: 0.16021  val_loss: 0.80957  val_acc: 0.78035  elapsed: 11m 45s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 9 - train_loss: 0.12214  val_loss: 0.81670  val_acc: 0.79191  elapsed: 13m 11s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 10 - train_loss: 0.10967  val_loss: 0.79363  val_acc: 0.79576  elapsed: 14m 40s\n================ ༼ つ ◕_◕ ༽つ Epoch 11 - train_loss: 0.10035  val_loss: 0.84573  val_acc: 0.79576  elapsed: 16m 10s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 12 - train_loss: 0.07013  val_loss: 0.90079  val_acc: 0.80347  elapsed: 17m 36s\n================ ༼ つ ◕_◕ ༽つ Epoch 13 - train_loss: 0.09059  val_loss: 0.86017  val_acc: 0.80347  elapsed: 19m 5s\n================ ༼ つ ◕_◕ ༽つ Epoch 14 - train_loss: 0.07040  val_loss: 0.94074  val_acc: 0.80347  elapsed: 20m 31s\n================ ༼ つ ◕_◕ ༽つ Epoch 15 - train_loss: 0.04287  val_loss: 0.95080  val_acc: 0.80347  elapsed: 21m 57s\n================ ༼ つ ◕_◕ ༽つ Epoch 16 - train_loss: 0.02883  val_loss: 1.06261  val_acc: 0.80347  elapsed: 23m 23s\n================ ༼ つ ◕_◕ ༽つ Epoch 17 - train_loss: 0.03049  val_loss: 0.96694  val_acc: 0.80347  elapsed: 24m 49s\n================ ༼ つ ◕_◕ ༽つ Epoch 18 - train_loss: 0.12792  val_loss: 0.95773  val_acc: 0.80347  elapsed: 26m 15s\n================ ༼ つ ◕_◕ ༽つ Epoch 19 - train_loss: 0.08054  val_loss: 0.97431  val_acc: 0.80347  elapsed: 27m 41s\n================ ༼ つ ◕_◕ ༽つ Epoch 20 - train_loss: 0.05291  val_loss: 1.05426  val_acc: 0.80347  elapsed: 29m 7s\n============== ༼ つ ◕_◕ ༽つ BEST epoch : 20, Accuracy : 0.8034682080924855 ====================================\n========================== ༼ つ ◕_◕ ༽つ Model Load 0_th FOLD =================================\npredict values check :  [-1.36245227 -1.86860836 -1.61639369 -1.80235422  6.18304968]\n2/5 fold training starts!\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 1 - train_loss: 1.32144  val_loss: 1.00266  val_acc: 0.65125  elapsed: 30m 40s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 2 - train_loss: 0.82322  val_loss: 0.66699  val_acc: 0.76493  elapsed: 32m 6s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 3 - train_loss: 0.54877  val_loss: 0.62584  val_acc: 0.77071  elapsed: 33m 36s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 4 - train_loss: 0.38880  val_loss: 0.62766  val_acc: 0.80154  elapsed: 35m 5s\n================ ༼ つ ◕_◕ ༽つ Epoch 5 - train_loss: 0.29040  val_loss: 0.69937  val_acc: 0.80154  elapsed: 36m 34s\n================ ༼ つ ◕_◕ ༽つ Epoch 6 - train_loss: 0.23955  val_loss: 0.70354  val_acc: 0.80154  elapsed: 38m 0s\n================ ༼ つ ◕_◕ ༽つ Epoch 7 - train_loss: 0.18920  val_loss: 0.77011  val_acc: 0.80154  elapsed: 39m 26s\n================ ༼ つ ◕_◕ ༽つ Epoch 8 - train_loss: 0.13679  val_loss: 0.77554  val_acc: 0.80154  elapsed: 40m 52s\n================ ༼ つ ◕_◕ ༽つ Epoch 9 - train_loss: 0.13474  val_loss: 0.83090  val_acc: 0.80154  elapsed: 42m 18s\n================ ༼ つ ◕_◕ ༽つ Epoch 10 - train_loss: 0.10079  val_loss: 0.88364  val_acc: 0.80154  elapsed: 43m 44s\n================ ༼ つ ◕_◕ ༽つ Epoch 11 - train_loss: 0.12408  val_loss: 0.95277  val_acc: 0.80154  elapsed: 45m 10s\n================ ༼ つ ◕_◕ ༽つ Epoch 12 - train_loss: 0.10022  val_loss: 0.90390  val_acc: 0.80154  elapsed: 46m 36s\n================ ༼ つ ◕_◕ ༽つ Epoch 13 - train_loss: 0.08630  val_loss: 0.94764  val_acc: 0.80154  elapsed: 48m 2s\n================ ༼ つ ◕_◕ ༽つ Epoch 14 - train_loss: 0.12250  val_loss: 1.06302  val_acc: 0.80154  elapsed: 49m 28s\n================ ༼ つ ◕_◕ ༽つ Epoch 15 - train_loss: 0.12783  val_loss: 1.03984  val_acc: 0.80154  elapsed: 50m 54s\n================ ༼ つ ◕_◕ ༽つ Epoch 16 - train_loss: 0.08846  val_loss: 0.92114  val_acc: 0.80154  elapsed: 52m 20s\n================ ༼ つ ◕_◕ ༽つ Epoch 17 - train_loss: 0.04863  val_loss: 1.09368  val_acc: 0.80154  elapsed: 53m 45s\n================ ༼ つ ◕_◕ ༽つ Epoch 18 - train_loss: 0.06977  val_loss: 0.90811  val_acc: 0.80154  elapsed: 55m 11s\n================ ༼ つ ◕_◕ ༽つ Epoch 19 - train_loss: 0.04807  val_loss: 0.96477  val_acc: 0.80154  elapsed: 56m 37s\n================ ༼ つ ◕_◕ ༽つ Epoch 20 - train_loss: 0.05075  val_loss: 1.06507  val_acc: 0.80154  elapsed: 58m 3s\n============== ༼ つ ◕_◕ ༽つ BEST epoch : 20, Accuracy : 0.8015414258188824 ====================================\n========================== ༼ つ ◕_◕ ༽つ Model Load 1_th FOLD =================================\npredict values check :  [-0.92885    -1.80002618 -1.37045491 -1.04935539  5.20982838]\n3/5 fold training starts!\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 1 - train_loss: 1.52491  val_loss: 1.50229  val_acc: 0.35521  elapsed: 59m 36s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 2 - train_loss: 1.50927  val_loss: 1.40160  val_acc: 0.46718  elapsed: 61m 2s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 3 - train_loss: 1.21077  val_loss: 1.10416  val_acc: 0.63127  elapsed: 62m 32s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 4 - train_loss: 0.87859  val_loss: 0.87905  val_acc: 0.70463  elapsed: 64m 1s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 5 - train_loss: 0.65069  val_loss: 0.69174  val_acc: 0.75869  elapsed: 65m 30s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 6 - train_loss: 0.48926  val_loss: 0.73700  val_acc: 0.77799  elapsed: 66m 59s\n================ ༼ つ ◕_◕ ༽つ Epoch 7 - train_loss: 0.34245  val_loss: 0.75237  val_acc: 0.77799  elapsed: 68m 29s\n================ ༼ つ ◕_◕ ༽つ Epoch 8 - train_loss: 0.27674  val_loss: 0.87270  val_acc: 0.77799  elapsed: 69m 55s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 9 - train_loss: 0.24765  val_loss: 0.79994  val_acc: 0.78378  elapsed: 71m 21s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 10 - train_loss: 0.16360  val_loss: 0.82246  val_acc: 0.78958  elapsed: 72m 50s\n================ ༼ つ ◕_◕ ༽つ Epoch 11 - train_loss: 0.12189  val_loss: 1.01405  val_acc: 0.78958  elapsed: 74m 19s\n================ ༼ つ ◕_◕ ༽つ Epoch 12 - train_loss: 0.14651  val_loss: 0.94448  val_acc: 0.78958  elapsed: 75m 45s\n================ ༼ つ ◕_◕ ༽つ Epoch 13 - train_loss: 0.10588  val_loss: 0.99741  val_acc: 0.78958  elapsed: 77m 11s\n================ ༼ つ ◕_◕ ༽つ Epoch 14 - train_loss: 0.07151  val_loss: 1.01192  val_acc: 0.78958  elapsed: 78m 37s\n================ ༼ つ ◕_◕ ༽つ Epoch 15 - train_loss: 0.07780  val_loss: 0.94599  val_acc: 0.78958  elapsed: 80m 3s\n================ ༼ つ ◕_◕ ༽つ Epoch 16 - train_loss: 0.11546  val_loss: 1.07729  val_acc: 0.78958  elapsed: 81m 29s\n================ ༼ つ ◕_◕ ༽つ Epoch 17 - train_loss: 0.10580  val_loss: 1.09616  val_acc: 0.78958  elapsed: 82m 55s\n================ ༼ つ ◕_◕ ༽つ Epoch 18 - train_loss: 0.09298  val_loss: 1.20661  val_acc: 0.78958  elapsed: 84m 21s\n================ ༼ つ ◕_◕ ༽つ Epoch 19 - train_loss: 0.09785  val_loss: 1.06084  val_acc: 0.78958  elapsed: 85m 47s\n================ ༼ つ ◕_◕ ༽つ Epoch 20 - train_loss: 0.08228  val_loss: 1.16224  val_acc: 0.78958  elapsed: 87m 13s\n============== ༼ つ ◕_◕ ༽つ BEST epoch : 20, Accuracy : 0.7895752895752896 ====================================\n========================== ༼ つ ◕_◕ ༽つ Model Load 2_th FOLD =================================\npredict values check :  [-1.15437675 -1.08320224 -1.41303551 -1.3001709   5.12705183]\n4/5 fold training starts!\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 1 - train_loss: 1.27429  val_loss: 0.98409  val_acc: 0.61390  elapsed: 88m 46s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 2 - train_loss: 0.81162  val_loss: 0.70669  val_acc: 0.75483  elapsed: 90m 13s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 3 - train_loss: 0.55252  val_loss: 0.67579  val_acc: 0.77413  elapsed: 91m 42s\n================ ༼ つ ◕_◕ ༽つ Epoch 4 - train_loss: 0.38334  val_loss: 0.68571  val_acc: 0.77413  elapsed: 93m 11s\n================ ༼ つ ◕_◕ ༽つ Epoch 5 - train_loss: 0.26200  val_loss: 0.79634  val_acc: 0.77413  elapsed: 94m 37s\n================ ༼ つ ◕_◕ ༽つ Epoch 6 - train_loss: 0.21434  val_loss: 0.79233  val_acc: 0.77413  elapsed: 96m 3s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 7 - train_loss: 0.14687  val_loss: 0.76584  val_acc: 0.79344  elapsed: 97m 29s\n================ ༼ つ ◕_◕ ༽つ Epoch 8 - train_loss: 0.12023  val_loss: 0.75634  val_acc: 0.79344  elapsed: 98m 59s\n================ ༼ つ ◕_◕ ༽つ Epoch 9 - train_loss: 0.10288  val_loss: 0.86782  val_acc: 0.79344  elapsed: 100m 24s\n================ ༼ つ ◕_◕ ༽つ Epoch 10 - train_loss: 0.10270  val_loss: 0.95275  val_acc: 0.79344  elapsed: 101m 50s\n================ ༼ つ ◕_◕ ༽つ Epoch 11 - train_loss: 0.08230  val_loss: 0.85315  val_acc: 0.79344  elapsed: 103m 16s\n================ ༼ つ ◕_◕ ༽つ Epoch 12 - train_loss: 0.07498  val_loss: 1.04924  val_acc: 0.79344  elapsed: 104m 42s\n================ ༼ つ ◕_◕ ༽つ Epoch 13 - train_loss: 0.08624  val_loss: 0.92606  val_acc: 0.79344  elapsed: 106m 8s\n================ ༼ つ ◕_◕ ༽つ Epoch 14 - train_loss: 0.05540  val_loss: 0.95118  val_acc: 0.79344  elapsed: 107m 34s\n================ ༼ つ ◕_◕ ༽つ Epoch 15 - train_loss: 0.07505  val_loss: 1.02787  val_acc: 0.79344  elapsed: 109m 0s\n================ ༼ つ ◕_◕ ༽つ Epoch 16 - train_loss: 0.05928  val_loss: 0.94063  val_acc: 0.79344  elapsed: 110m 26s\n================ ༼ つ ◕_◕ ༽つ Epoch 17 - train_loss: 0.04590  val_loss: 0.94051  val_acc: 0.79344  elapsed: 111m 52s\n================ ༼ つ ◕_◕ ༽つ Epoch 18 - train_loss: 0.02196  val_loss: 1.03993  val_acc: 0.79344  elapsed: 113m 18s\n================ ༼ つ ◕_◕ ༽つ Epoch 19 - train_loss: 0.10046  val_loss: 1.01677  val_acc: 0.79344  elapsed: 114m 45s\n================ ༼ つ ◕_◕ ༽つ Epoch 20 - train_loss: 0.05202  val_loss: 0.98841  val_acc: 0.79344  elapsed: 116m 11s\n============== ༼ つ ◕_◕ ༽つ BEST epoch : 20, Accuracy : 0.7934362934362934 ====================================\n========================== ༼ つ ◕_◕ ༽つ Model Load 3_th FOLD =================================\npredict values check :  [-1.18726242 -1.64108956 -2.56106138 -0.81890541  5.86428928]\n5/5 fold training starts!\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 1 - train_loss: 1.29307  val_loss: 1.11968  val_acc: 0.56950  elapsed: 117m 44s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 2 - train_loss: 0.86789  val_loss: 0.83234  val_acc: 0.69112  elapsed: 119m 10s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 3 - train_loss: 0.61719  val_loss: 0.71969  val_acc: 0.74131  elapsed: 120m 40s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 4 - train_loss: 0.46249  val_loss: 0.71162  val_acc: 0.76641  elapsed: 122m 9s\n================ ༼ つ ◕_◕ ༽つ Epoch 5 - train_loss: 0.33137  val_loss: 0.75445  val_acc: 0.76641  elapsed: 123m 38s\n================ ༼ つ ◕_◕ ༽つ Epoch 6 - train_loss: 0.24466  val_loss: 0.92840  val_acc: 0.76641  elapsed: 125m 4s\n================ ༼ つ ◕_◕ ༽つ Epoch 7 - train_loss: 0.18946  val_loss: 0.89260  val_acc: 0.76641  elapsed: 126m 30s\n================ ༼ つ ◕_◕ ༽つ Epoch 8 - train_loss: 0.12453  val_loss: 0.94235  val_acc: 0.76641  elapsed: 127m 56s\n================ ༼ つ ◕_◕ ༽つ Epoch 9 - train_loss: 0.09945  val_loss: 1.06524  val_acc: 0.76641  elapsed: 129m 22s\n================ ༼ つ ◕_◕ ༽つ Epoch 10 - train_loss: 0.11608  val_loss: 1.12537  val_acc: 0.76641  elapsed: 130m 48s\n================ ༼ つ ◕_◕ ༽つ Epoch 11 - train_loss: 0.10894  val_loss: 1.03758  val_acc: 0.76641  elapsed: 132m 14s\n================ ༼ つ ◕_◕ ༽つ Epoch 12 - train_loss: 0.10999  val_loss: 1.17306  val_acc: 0.76641  elapsed: 133m 40s\n================ ༼ つ ◕_◕ ༽つ Epoch 13 - train_loss: 0.06972  val_loss: 1.12615  val_acc: 0.76641  elapsed: 135m 6s\n================ ༼ つ ◕_◕ ༽つ Epoch 14 - train_loss: 0.05343  val_loss: 1.17774  val_acc: 0.76641  elapsed: 136m 32s\n================ ༼ つ ◕_◕ ༽つ Epoch 15 - train_loss: 0.04043  val_loss: 1.14962  val_acc: 0.76641  elapsed: 137m 58s\n================ ༼ つ ◕_◕ ༽つ Epoch 16 - train_loss: 0.07620  val_loss: 1.28777  val_acc: 0.76641  elapsed: 139m 23s\n================ ༼ つ ◕_◕ ༽つ Epoch 17 - train_loss: 0.04264  val_loss: 1.24419  val_acc: 0.76641  elapsed: 140m 49s\n================ ༼ つ ◕_◕ ༽つ Epoch 18 - train_loss: 0.05794  val_loss: 1.23768  val_acc: 0.76641  elapsed: 142m 15s\n================ ༼ つ ◕_◕ ༽つ Epoch 19 - train_loss: 0.03949  val_loss: 1.34056  val_acc: 0.76641  elapsed: 143m 41s\n================ ༼ つ ◕_◕ ༽つ Epoch 20 - train_loss: 0.07270  val_loss: 1.15647  val_acc: 0.76641  elapsed: 145m 7s\n============== ༼ つ ◕_◕ ༽つ BEST epoch : 20, Accuracy : 0.7664092664092664 ====================================\n========================== ༼ つ ◕_◕ ༽つ Model Load 4_th FOLD =================================\npredict values check :  [-0.16869876 -0.89300603 -1.75426686 -1.55165482  5.00188446]\nCPU times: user 2h 16min 44s, sys: 7min 37s, total: 2h 24min 21s\nWall time: 2h 25min 12s\n"
    }
   ],
   "source": [
    "%%time\n",
    "best_epoch_list = []\n",
    "best_val_acc_list = []\n",
    "start_time = time()\n",
    "\n",
    "splits = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED).split(x_train, y_train))\n",
    "for fold in [0, 1, 2, 3, 4]:\n",
    "\n",
    "    print(\"{}/5 fold training starts!\".format(fold+1))\n",
    "    \n",
    "    fold_num = str(fold + 1)\n",
    "\n",
    "    trn_index, val_index = splits[fold]\n",
    "\n",
    "    X_train, X_valid = x_train[trn_index], x_train[val_index]\n",
    "    #train_length, valid_length = lengths[trn_index], lengths[val_index]\n",
    "    Y_train, Y_valid = y_train[trn_index], y_train[val_index]\n",
    "\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype = torch.long), torch.tensor(Y_train, dtype=torch.long)) #TensorDataset(X_train, train_length, torch.tensor(Y_train))\n",
    "    valid_dataset = TensorDataset(torch.tensor(X_valid, dtype = torch.long), torch.tensor(Y_valid, dtype=torch.long)) #TensorDataset(X_valid, valid_length, torch.tensor(Y_valid))\n",
    "    \n",
    "    model = BertForSequenceClassification.from_pretrained(\"../job_nlp/working\",cache_dir=None, num_labels=5)\n",
    "    model.zero_grad()\n",
    "    model = model.to(device)\n",
    "    #optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "    #                 lr=lr,\n",
    "    #                 warmup=0.05,\n",
    "    #                 t_total=num_train_optimization_steps)\n",
    "    #scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    \n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "    #train = train_dataset\n",
    "\n",
    "    num_train_optimization_steps = int(EPOCHS*len(train_dataset)/batch_size/accumulation_steps)\n",
    "\n",
    "    #optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "    #                     lr=lr,\n",
    "    #                     warmup=0.05,\n",
    "    #                     t_total=np.ceil(num_train_optimization_steps))\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr, weight_decay=0.000025)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "    best_valid_score = 0\n",
    "    best_val_acc = 0\n",
    "    \n",
    "    #tq = tqdm_notebook(range(EPOCHS))\n",
    "    #model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\",verbosity=0)\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        \n",
    "        #start_time = time.time()\n",
    "        train_loss = 0\n",
    "        train_total_correct = 0\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        #tk0 = tqdm_notebook(enumerate(train_loader),total=len(train_loader),leave=False)\n",
    "        \n",
    "        for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            preds  = model(x_batch.to(device), attention_mask = (x_batch>0).to(device), labels=None)\n",
    "            loss = criterion(preds, y_batch.to(device))\n",
    "            loss.backward()\n",
    "            \n",
    "            if (i+1) % accumulation_steps == 0:             # Wait for several backward steps\n",
    "                optimizer.step()                            # Now we can do an optimizer step\n",
    "                optimizer.zero_grad()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            train_loss += loss.item()/len(train_loader)\n",
    "            \n",
    "        # Validation Starts\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        valid_total_correct = 0\n",
    "        \n",
    "        #valid_preds = np.zeros(len(valid_dataset),5)\n",
    "        #valid_targets = np.zeros(len(valid_dataset),5)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "                #valid_targets[i*batch_size: (i+1)*batch_size] = y_batch.numpy().copy()\n",
    "                \n",
    "                preds = model(x_batch.to(device), attention_mask = (x_batch>0).to(device), labels=None)\n",
    "                \n",
    "                loss = criterion(preds, y_batch.to(device))\n",
    "                \n",
    "                output_prob = F.softmax(preds, dim=1)\n",
    "\n",
    "                predict_vector = np.argmax(to_numpy(output_prob), axis=1)\n",
    "                label_vector = to_numpy(y_batch)\n",
    "                #valid_preds[i*batch_size: (i+1)*batch_size] = np.argmax(preds_prob.detach().cpu().squeeze().numpy())\n",
    "                bool_vector = predict_vector == label_vector\n",
    "                \n",
    "                val_loss += loss.item()/len(valid_loader)\n",
    "                valid_total_correct += bool_vector.sum()\n",
    "        \n",
    "        #val_score = roc_auc_score(valid_targets, valid_preds)\n",
    "\n",
    "        elapsed = time() - start_time\n",
    "        val_acc = valid_total_correct / len(valid_loader.dataset)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch\n",
    "            print(\"val_acc has improved !! \")\n",
    "            best_epoch_list.append(best_epoch)\n",
    "            best_val_acc_list.append(best_val_acc)\n",
    "            \n",
    "            torch.save(model.state_dict(), '../job_nlp/Bert_20e_maxseq400_fold_{}.pt'.format(fold))\n",
    "            #print(\"================ ༼ つ ◕_◕ ༽つ BEST epoch : {}, Accuracy : {} \".format(epoch, best_val_acc))\n",
    "            \n",
    "        #lr = [_['lr'] for _ in optimizer.param_g] # or optimizer\n",
    "        print(\"================ ༼ つ ◕_◕ ༽つ Epoch {} - train_loss: {:.5f}  val_loss: {:.5f}  val_acc: {:.5f}  elapsed: {:.0f}m {:.0f}s\".format(epoch, train_loss, val_loss, best_val_acc, elapsed // 60, elapsed % 60))\n",
    "    print(\"============== ༼ つ ◕_◕ ༽つ BEST epoch : {}, Accuracy : {} ====================================\".format(epoch, best_val_acc))\n",
    "    #best_epoch_list.append(best_epoch)\n",
    "    #best_val_acc_list.append(best_val_acc)\n",
    "    \n",
    "    #---- Inference ----\n",
    "    #batch_size = 8\n",
    "\n",
    "    print(\"========================== ༼ つ ◕_◕ ༽つ Model Load {}_th FOLD =================================\".format(fold))\n",
    "    model.load_state_dict(torch.load('Bert_20e_maxseq400_fold_{}.pt'.format(fold)))\n",
    "    model.eval()\n",
    "    predictions = np.zeros((len(test_loader.dataset),5))\n",
    "    with torch.no_grad():\n",
    "        for i, (x_batch, ) in enumerate(test_loader):\n",
    "            preds = model(x_batch.to(device), attention_mask = (x_batch>0).to(device), labels=None)\n",
    "            \n",
    "            predictions[i*batch_size: (i+1)*batch_size] = to_numpy(preds)\n",
    "    print(\"predict values check : \",predictions[0])\n",
    "    np.savetxt(\"../job_nlp/bert_raw_submission/bert_20e_maxseq400_fold_{}.csv\".format(fold), predictions, delimiter=\",\")   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}