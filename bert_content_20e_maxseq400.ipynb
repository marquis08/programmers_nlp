{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from time import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "import datetime\n",
    "import pkg_resources\n",
    "#import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import gc\n",
    "import re\n",
    "import operator \n",
    "import sys\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, Subset, DataLoader\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "#from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "#%matplotlib inline\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import os\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "import pickle\n",
    "#from apex import amp\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=123):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class AdamW(Optimizer):\n",
    "    \"\"\"Implements AdamW algorithm.\n",
    "\n",
    "    It has been proposed in `Fixing Weight Decay Regularization in Adam`_.\n",
    "\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float, optional): learning rate (default: 1e-3)\n",
    "        betas (Tuple[float, float], optional): coefficients used for computing\n",
    "            running averages of gradient and its square (default: (0.9, 0.999))\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability (default: 1e-8)\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "\n",
    "    .. Fixing Weight Decay Regularization in Adam:\n",
    "    https://arxiv.org/abs/1711.05101\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay)\n",
    "        super(AdamW, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('AdamW does not support sparse gradients, please consider SparseAdam instead')\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                # according to the paper, this penalty should come after the bias correction\n",
    "                # if group['weight_decay'] != 0:\n",
    "                #     grad = grad.add(group['weight_decay'], p.data)\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "\n",
    "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p.data.add_(-group['weight_decay'], p.data)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 295 \n",
    "SEED = 42\n",
    "EPOCHS = 20\n",
    "Data_dir=\"../job_nlp/\"\n",
    "WORK_DIR = \"../job_nlp/working/\"\n",
    "#num_to_load=100000                         #Train size to match time limit\n",
    "#valid_size= 50000                          #Validation Size\n",
    "TARGET = 'smishing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/matsuik/ppbert\n",
    "package_dir_a = \"../job_nlp/ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT\"\n",
    "sys.path.insert(0, package_dir_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/home/yilgukseo/anaconda3/envs/pytorch/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n  return f(*args, **kwds)\n/home/yilgukseo/anaconda3/envs/pytorch/lib/python3.7/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n  return f(*args, **kwds)\n/home/yilgukseo/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py:5879: ResourceWarning: unclosed file <_io.TextIOWrapper name='/home/yilgukseo/.keras/keras.json' mode='r' encoding='UTF-8'>\n  _config = json.load(open(_config_path))\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
    }
   ],
   "source": [
    "from pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification,BertAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "erNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_9/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_9/output/dense/kernel with shape [3072, 768]\nLoading TF weight bert/pooler/dense/bias with shape [768]\nLoading TF weight bert/pooler/dense/kernel with shape [768, 768]\nLoading TF weight cls/predictions/output_bias with shape [30522]\nLoading TF weight cls/predictions/transform/LayerNorm/beta with shape [768]\nLoading TF weight cls/predictions/transform/LayerNorm/gamma with shape [768]\nLoading TF weight cls/predictions/transform/dense/bias with shape [768]\nLoading TF weight cls/predictions/transform/dense/kernel with shape [768, 768]\nLoading TF weight cls/seq_relationship/output_bias with shape [2]\nLoading TF weight cls/seq_relationship/output_weights with shape [2, 768]\nInitialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\nInitialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\nInitialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\nInitialize PyTorch weight ['cls', 'predictions', 'output_bias']\nInitialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'bias']\nInitialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'kernel']\nInitialize PyTorch weight ['cls', 'seq_relationship', 'output_bias']\nInitialize PyTorch weight ['cls', 'seq_relationship', 'output_weights']\nSave PyTorch model to ../job_nlp/working/pytorch_model.bin\n"
    },
    {
     "data": {
      "text/plain": "'../job_nlp/working/bert_config.json'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Translate model from tensorflow to pytorch\n",
    "BERT_MODEL_PATH = '../job_nlp/bert-pretrained-models/uncased_L-12_H-768_A-12/uncased_L-12_H-768_A-12/'\n",
    "convert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n",
    "    BERT_MODEL_PATH + 'bert_model.ckpt',\n",
    "BERT_MODEL_PATH + 'bert_config.json',\n",
    "WORK_DIR + 'pytorch_model.bin')\n",
    "\n",
    "shutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', WORK_DIR + 'bert_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the Bert configuration file\n",
    "from pytorch_pretrained_bert import BertConfig\n",
    "\n",
    "bert_config = BertConfig('../job_nlp/bert-pretrained-models/uncased_L-12_H-768_A-12/uncased_L-12_H-768_A-12/'+'bert_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"max_position_embeddings\": 512,\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 30522\n}"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the lines to BERT format\n",
    "# Thanks to https://www.kaggle.com/httpwwwfszyc/bert-in-keras-taming\n",
    "def convert_lines(example, max_seq_length,tokenizer):\n",
    "    max_seq_length -=2\n",
    "    all_tokens = []\n",
    "    longer = 0\n",
    "    for text in tqdm_notebook(example):\n",
    "        tokens_a = tokenizer.tokenize(text)\n",
    "        if len(tokens_a)>max_seq_length:\n",
    "            tokens_a = tokens_a[:max_seq_length]\n",
    "            longer += 1\n",
    "        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n",
    "        all_tokens.append(one_token)\n",
    "    print(longer)\n",
    "    return np.array(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_PATH = '../job_nlp/bert-pretrained-models/uncased_L-12_H-768_A-12/uncased_L-12_H-768_A-12/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 31.1 ms, sys: 2.45 ms, total: 33.6 ms\nWall time: 33.3 ms\n"
    }
   ],
   "source": [
    "%%time\n",
    "train_df = pd.read_csv(os.path.join(Data_dir,\"train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(Data_dir,\"test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "3    0.354552\n5    0.248457\n4    0.184414\n2    0.113812\n1    0.098765\nName: label, dtype: float64"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "title      [$$$] 재귀함수를 처리하면서 setTimeout으로 딜레이를 주면 읽는 순서가 ...\ncontent                                                  NaN\nlabel                                                      4\nName: 2400, dtype: object\n"
    }
   ],
   "source": [
    "# replace NaN\n",
    "print(train_df.iloc[2400,:])\n",
    "train_df.iloc[2400,1] = \"@@@\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 38.1 ms, sys: 0 ns, total: 38.1 ms\nWall time: 37.7 ms\n"
    }
   ],
   "source": [
    "%%time\n",
    "train_df['text'] = train_df[['title', 'content']].apply(lambda x: ' '.join(x), axis = 1)\n",
    "test_df['text'] = test_df[['title', 'content']].apply(lambda x: ' '.join(x), axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>content</th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>wait()과 sleep()의 차이점은 뭔가요</td>\n      <td>###발생하는 문제 및 실행환경\\nwait()과 sleep()의 차이점은 뭔가요</td>\n      <td>3</td>\n      <td>wait()과 sleep()의 차이점은 뭔가요 ###발생하는 문제 및 실행환경\\nw...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>$$$ 초보 외부 클래스 멤버 변수 사용 질문합니다.ㅠㅠ</td>\n      <td>1.헤더에, 사용할 멤버변수가 담긴 헤더 파일이 Include 되어있습니다.\\n예를...</td>\n      <td>2</td>\n      <td>$$$ 초보 외부 클래스 멤버 변수 사용 질문합니다.ㅠㅠ 1.헤더에, 사용할 멤버변...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ORM: Sequelize: 다대다 관계 쿼리</td>\n      <td>안녕하세요.\\n\\n어떻게 다대다 관계 쿼리를 해야하나요? 예를들어, `product...</td>\n      <td>4</td>\n      <td>ORM: Sequelize: 다대다 관계 쿼리 안녕하세요.\\n\\n어떻게 다대다 관계...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>$$$에서 숫자가 NaN인지 검사하려면 어떻게해야하죠?</td>\n      <td>```\\nparseFloat('geoff') == NaN;\\n\\nparseFloat...</td>\n      <td>4</td>\n      <td>$$$에서 숫자가 NaN인지 검사하려면 어떻게해야하죠? ```\\nparseFloat...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>$$$ 2.7에서 중국어, 특수문자 인코딩 하는 방법이 뭔가요?</td>\n      <td>```\\n&gt;&gt;&gt;dict['name']\\n胡安·马塔\\n&gt;&gt;&gt;json.dumps(dic...</td>\n      <td>5</td>\n      <td>$$$ 2.7에서 중국어, 특수문자 인코딩 하는 방법이 뭔가요? ```\\n&gt;&gt;&gt;di...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                 title  \\\n0            wait()과 sleep()의 차이점은 뭔가요   \n1      $$$ 초보 외부 클래스 멤버 변수 사용 질문합니다.ㅠㅠ   \n2            ORM: Sequelize: 다대다 관계 쿼리   \n3       $$$에서 숫자가 NaN인지 검사하려면 어떻게해야하죠?   \n4  $$$ 2.7에서 중국어, 특수문자 인코딩 하는 방법이 뭔가요?   \n\n                                             content  label  \\\n0       ###발생하는 문제 및 실행환경\\nwait()과 sleep()의 차이점은 뭔가요      3   \n1  1.헤더에, 사용할 멤버변수가 담긴 헤더 파일이 Include 되어있습니다.\\n예를...      2   \n2  안녕하세요.\\n\\n어떻게 다대다 관계 쿼리를 해야하나요? 예를들어, `product...      4   \n3  ```\\nparseFloat('geoff') == NaN;\\n\\nparseFloat...      4   \n4  ```\\n>>>dict['name']\\n胡安·马塔\\n>>>json.dumps(dic...      5   \n\n                                                text  \n0  wait()과 sleep()의 차이점은 뭔가요 ###발생하는 문제 및 실행환경\\nw...  \n1  $$$ 초보 외부 클래스 멤버 변수 사용 질문합니다.ㅠㅠ 1.헤더에, 사용할 멤버변...  \n2  ORM: Sequelize: 다대다 관계 쿼리 안녕하세요.\\n\\n어떻게 다대다 관계...  \n3  $$$에서 숫자가 NaN인지 검사하려면 어떻게해야하죠? ```\\nparseFloat...  \n4  $$$ 2.7에서 중국어, 특수문자 인코딩 하는 방법이 뭔가요? ```\\n>>>di...  "
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "25"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.iloc[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "43"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.iloc[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "69"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.iloc[0,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['t_length'] = train_df['title'].apply(lambda x: len(x))\n",
    "train_df['c_length'] = train_df['content'].apply(lambda x: len(x))\n",
    "train_df['text_length'] = train_df['text'].apply(lambda x: len(x))\n",
    "\n",
    "test_df['t_length'] = test_df['title'].apply(lambda x: len(x))\n",
    "test_df['c_length'] = test_df['content'].apply(lambda x: len(x))\n",
    "test_df['text_length'] = test_df['text'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>t_length</th>\n      <th>c_length</th>\n      <th>text_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>2592.000000</td>\n      <td>2592.000000</td>\n      <td>2592.000000</td>\n      <td>2592.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>3.369985</td>\n      <td>29.945988</td>\n      <td>891.706404</td>\n      <td>922.652392</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.245321</td>\n      <td>15.235909</td>\n      <td>1684.607332</td>\n      <td>1685.667323</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>3.000000</td>\n      <td>8.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>3.000000</td>\n      <td>19.000000</td>\n      <td>180.000000</td>\n      <td>208.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>3.000000</td>\n      <td>27.000000</td>\n      <td>385.000000</td>\n      <td>418.500000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>4.000000</td>\n      <td>37.000000</td>\n      <td>896.000000</td>\n      <td>920.750000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>5.000000</td>\n      <td>119.000000</td>\n      <td>28756.000000</td>\n      <td>28780.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "             label     t_length      c_length   text_length\ncount  2592.000000  2592.000000   2592.000000   2592.000000\nmean      3.369985    29.945988    891.706404    922.652392\nstd       1.245321    15.235909   1684.607332   1685.667323\nmin       1.000000     3.000000      3.000000      8.000000\n25%       3.000000    19.000000    180.000000    208.000000\n50%       3.000000    27.000000    385.000000    418.500000\n75%       4.000000    37.000000    896.000000    920.750000\nmax       5.000000   119.000000  28756.000000  28780.000000"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>t_length</th>\n      <th>c_length</th>\n      <th>text_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>500.000000</td>\n      <td>500.000000</td>\n      <td>500.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>29.310000</td>\n      <td>858.270000</td>\n      <td>888.580000</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>14.786046</td>\n      <td>1957.436752</td>\n      <td>1956.747171</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>5.000000</td>\n      <td>20.000000</td>\n      <td>53.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>19.750000</td>\n      <td>192.250000</td>\n      <td>227.250000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>26.500000</td>\n      <td>371.000000</td>\n      <td>399.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>36.000000</td>\n      <td>841.250000</td>\n      <td>870.500000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>134.000000</td>\n      <td>35210.000000</td>\n      <td>35226.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "         t_length      c_length   text_length\ncount  500.000000    500.000000    500.000000\nmean    29.310000    858.270000    888.580000\nstd     14.786046   1957.436752   1956.747171\nmin      5.000000     20.000000     53.000000\n25%     19.750000    192.250000    227.250000\n50%     26.500000    371.000000    399.000000\n75%     36.000000    841.250000    870.500000\nmax    134.000000  35210.000000  35226.000000"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "loaded 2592 records\n/home/yilgukseo/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  import sys\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa66bd30baa4b9487cd1f0001afec39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=2592.0), HTML(value='')))"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n974\nX_train : 2592\nloaded 500 records\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d96d7829a24121b040d5797a20958f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=500.0), HTML(value='')))"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n188\nX_test : 500\n"
    }
   ],
   "source": [
    "#%%time\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)\n",
    "#train_df = pd.read_csv(os.path.join(Data_dir,\"train.csv\"))#.sample(num_to_load+valid_size,random_state=SEED)\n",
    "print('loaded %d records' % len(train_df))\n",
    "# Make sure all comment_text values are strings\n",
    "train_df['content'] = train_df['content'].astype(str) \n",
    "x_train = convert_lines(train_df[\"content\"].fillna(\"DUMMY_VALUE\"),MAX_SEQUENCE_LENGTH,tokenizer)\n",
    "print(\"X_train : {}\".format(len(x_train)))\n",
    "\n",
    "#test_df = pd.read_csv(os.path.join(Data_dir,\"public_test.csv\"))#.sample(num_to_load+valid_size,random_state=SEED)\n",
    "print('loaded %d records' % len(test_df))\n",
    "test_df['content'] = test_df['content'].astype(str) \n",
    "x_test = convert_lines(test_df[\"content\"].fillna(\"DUMMY_VALUE\"),MAX_SEQUENCE_LENGTH,tokenizer)\n",
    "print(\"X_test : {}\".format(len(x_test)))\n",
    "\n",
    "train_df=train_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# above not working in linux ?? these x_train & x_test are obtained from windows\n",
    "#x_train = np.loadtxt('../job_nlp/x_train.csv', delimiter=',')\n",
    "#x_test = np.loadtxt('../job_nlp/x_test.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(SEED)\n",
    "output_model_file = \"bert_pytorch.bin\"\n",
    "\n",
    "lr=2e-5\n",
    "batch_size = 8\n",
    "accumulation_steps=2\n",
    "n_labels = 2\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "TARGET = 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[TARGET] = train_df[TARGET]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([2, 1, 3,  ..., 0, 4, 1])"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x_train = train_df['text']\n",
    "y_train = torch.tensor(train_df[TARGET])#.long()\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([2, 1, 3, 3, 4])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(x):\n",
    "    return x.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(torch.tensor(x_test, dtype = torch.long)) #TensorDataset(X_valid, valid_length, torch.tensor(Y_valid))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"../job_nlp/working\",cache_dir=None, num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "================ ༼ つ ◕_◕ ༽つ 1/5 fold training starts!\n/home/yilgukseo/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 1 - train_loss: 1.26997  val_loss: 0.95356  val_acc: 0.64162  elapsed: 1m 28s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 2 - train_loss: 0.86164  val_loss: 0.72513  val_acc: 0.74952  elapsed: 2m 54s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 3 - train_loss: 0.59586  val_loss: 0.65616  val_acc: 0.77264  elapsed: 4m 23s\n================ ༼ つ ◕_◕ ༽つ Epoch 4 - train_loss: 0.46959  val_loss: 0.69946  val_acc: 0.77264  elapsed: 5m 52s\n================ ༼ つ ◕_◕ ༽つ Epoch 5 - train_loss: 0.33286  val_loss: 0.67387  val_acc: 0.77264  elapsed: 7m 18s\n================ ༼ つ ◕_◕ ༽つ Epoch 6 - train_loss: 0.24191  val_loss: 0.94004  val_acc: 0.77264  elapsed: 8m 44s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 7 - train_loss: 0.17543  val_loss: 0.83540  val_acc: 0.77457  elapsed: 10m 10s\n================ ༼ つ ◕_◕ ༽つ Epoch 8 - train_loss: 0.12882  val_loss: 0.93321  val_acc: 0.77457  elapsed: 11m 39s\n================ ༼ つ ◕_◕ ༽つ Epoch 9 - train_loss: 0.12237  val_loss: 0.94471  val_acc: 0.77457  elapsed: 13m 4s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 10 - train_loss: 0.09056  val_loss: 0.87780  val_acc: 0.78035  elapsed: 14m 30s\n================ ༼ つ ◕_◕ ༽つ Epoch 11 - train_loss: 0.09019  val_loss: 0.89897  val_acc: 0.78035  elapsed: 15m 59s\n================ ༼ つ ◕_◕ ༽つ Epoch 12 - train_loss: 0.09022  val_loss: 0.95272  val_acc: 0.78035  elapsed: 17m 25s\n================ ༼ つ ◕_◕ ༽つ Epoch 13 - train_loss: 0.08153  val_loss: 0.98328  val_acc: 0.78035  elapsed: 18m 51s\n================ ༼ つ ◕_◕ ༽つ Epoch 14 - train_loss: 0.07197  val_loss: 1.16162  val_acc: 0.78035  elapsed: 20m 17s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 15 - train_loss: 0.05762  val_loss: 1.02941  val_acc: 0.78998  elapsed: 21m 42s\n================ ༼ つ ◕_◕ ༽つ Epoch 16 - train_loss: 0.06961  val_loss: 1.15094  val_acc: 0.78998  elapsed: 23m 11s\n================ ༼ つ ◕_◕ ༽つ Epoch 17 - train_loss: 0.05915  val_loss: 1.11618  val_acc: 0.78998  elapsed: 24m 37s\n================ ༼ つ ◕_◕ ༽つ Epoch 18 - train_loss: 0.05215  val_loss: 1.12977  val_acc: 0.78998  elapsed: 26m 3s\n================ ༼ つ ◕_◕ ༽つ Epoch 19 - train_loss: 0.05726  val_loss: 1.03090  val_acc: 0.78998  elapsed: 27m 29s\n================ ༼ つ ◕_◕ ༽つ Epoch 20 - train_loss: 0.07749  val_loss: 1.07535  val_acc: 0.78998  elapsed: 28m 55s\n============== ༼ つ ◕_◕ ༽つ BEST epoch : 20, Accuracy : 0.789980732177264 ====================================\n========================== ༼ つ ◕_◕ ༽つ Model Load 0_th FOLD =================================\npredict values check :  [-1.33655119 -1.64948583 -1.99604511 -1.74130619  6.22682905]\n================ ༼ つ ◕_◕ ༽つ 2/5 fold training starts!\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 1 - train_loss: 1.31722  val_loss: 1.00038  val_acc: 0.66281  elapsed: 30m 28s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 2 - train_loss: 0.82660  val_loss: 0.66793  val_acc: 0.77264  elapsed: 31m 54s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 3 - train_loss: 0.56479  val_loss: 0.66947  val_acc: 0.78035  elapsed: 33m 23s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 4 - train_loss: 0.42481  val_loss: 0.65005  val_acc: 0.78805  elapsed: 34m 51s\n================ ༼ つ ◕_◕ ༽つ Epoch 5 - train_loss: 0.31143  val_loss: 0.69951  val_acc: 0.78805  elapsed: 36m 20s\n================ ༼ つ ◕_◕ ༽つ Epoch 6 - train_loss: 0.24120  val_loss: 0.73232  val_acc: 0.78805  elapsed: 37m 46s\n================ ༼ つ ◕_◕ ༽つ Epoch 7 - train_loss: 0.17967  val_loss: 0.99080  val_acc: 0.78805  elapsed: 39m 12s\n================ ༼ つ ◕_◕ ༽つ Epoch 8 - train_loss: 0.16717  val_loss: 0.79054  val_acc: 0.78805  elapsed: 40m 38s\n================ ༼ つ ◕_◕ ༽つ Epoch 9 - train_loss: 0.11300  val_loss: 0.85392  val_acc: 0.78805  elapsed: 42m 4s\n================ ༼ つ ◕_◕ ༽つ Epoch 10 - train_loss: 0.11132  val_loss: 1.04922  val_acc: 0.78805  elapsed: 43m 30s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 11 - train_loss: 0.13431  val_loss: 0.83856  val_acc: 0.79191  elapsed: 44m 56s\n================ ༼ つ ◕_◕ ༽つ Epoch 12 - train_loss: 0.08946  val_loss: 1.05974  val_acc: 0.79191  elapsed: 46m 25s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 13 - train_loss: 0.08668  val_loss: 0.97704  val_acc: 0.79576  elapsed: 47m 51s\n================ ༼ つ ◕_◕ ༽つ Epoch 14 - train_loss: 0.02919  val_loss: 1.11555  val_acc: 0.79576  elapsed: 49m 20s\n================ ༼ つ ◕_◕ ༽つ Epoch 15 - train_loss: 0.03958  val_loss: 0.98772  val_acc: 0.79576  elapsed: 50m 45s\n================ ༼ つ ◕_◕ ༽つ Epoch 16 - train_loss: 0.08741  val_loss: 1.12351  val_acc: 0.79576  elapsed: 52m 11s\n================ ༼ つ ◕_◕ ༽つ Epoch 17 - train_loss: 0.12820  val_loss: 1.03809  val_acc: 0.79576  elapsed: 53m 37s\n================ ༼ つ ◕_◕ ༽つ Epoch 18 - train_loss: 0.07281  val_loss: 1.00135  val_acc: 0.79576  elapsed: 55m 3s\n================ ༼ つ ◕_◕ ༽つ Epoch 19 - train_loss: 0.05741  val_loss: 1.05405  val_acc: 0.79576  elapsed: 56m 29s\n================ ༼ つ ◕_◕ ༽つ Epoch 20 - train_loss: 0.03647  val_loss: 1.04437  val_acc: 0.79576  elapsed: 57m 55s\n============== ༼ つ ◕_◕ ༽つ BEST epoch : 20, Accuracy : 0.7957610789980732 ====================================\n========================== ༼ つ ◕_◕ ༽つ Model Load 1_th FOLD =================================\npredict values check :  [-1.29721546 -1.60733271 -1.11734426 -1.99168658  6.62477732]\n================ ༼ つ ◕_◕ ༽つ 3/5 fold training starts!\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 1 - train_loss: 1.35356  val_loss: 1.03713  val_acc: 0.59846  elapsed: 59m 28s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 2 - train_loss: 0.91478  val_loss: 0.94048  val_acc: 0.68533  elapsed: 60m 54s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 3 - train_loss: 0.66772  val_loss: 0.80805  val_acc: 0.72394  elapsed: 62m 23s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 4 - train_loss: 0.47075  val_loss: 0.65813  val_acc: 0.76641  elapsed: 63m 52s\n================ ༼ つ ◕_◕ ༽つ Epoch 5 - train_loss: 0.36629  val_loss: 0.85477  val_acc: 0.76641  elapsed: 65m 21s\n================ ༼ つ ◕_◕ ༽つ Epoch 6 - train_loss: 0.30021  val_loss: 0.82802  val_acc: 0.76641  elapsed: 66m 46s\n================ ༼ つ ◕_◕ ༽つ Epoch 7 - train_loss: 0.23044  val_loss: 0.95205  val_acc: 0.76641  elapsed: 68m 12s\n================ ༼ つ ◕_◕ ༽つ Epoch 8 - train_loss: 0.18205  val_loss: 1.00494  val_acc: 0.76641  elapsed: 69m 38s\n================ ༼ つ ◕_◕ ༽つ Epoch 9 - train_loss: 0.14957  val_loss: 0.97451  val_acc: 0.76641  elapsed: 71m 4s\n================ ༼ つ ◕_◕ ༽つ Epoch 10 - train_loss: 0.15032  val_loss: 0.95657  val_acc: 0.76641  elapsed: 72m 30s\n================ ༼ つ ◕_◕ ༽つ Epoch 11 - train_loss: 0.11209  val_loss: 1.12512  val_acc: 0.76641  elapsed: 73m 56s\n================ ༼ つ ◕_◕ ༽つ Epoch 12 - train_loss: 0.11358  val_loss: 1.21408  val_acc: 0.76641  elapsed: 75m 21s\n================ ༼ つ ◕_◕ ༽つ Epoch 13 - train_loss: 0.10051  val_loss: 1.08425  val_acc: 0.76641  elapsed: 76m 47s\n================ ༼ つ ◕_◕ ༽つ Epoch 14 - train_loss: 0.08168  val_loss: 1.29008  val_acc: 0.76641  elapsed: 78m 13s\n================ ༼ つ ◕_◕ ༽つ Epoch 15 - train_loss: 0.08096  val_loss: 1.17141  val_acc: 0.76641  elapsed: 79m 39s\n================ ༼ つ ◕_◕ ༽つ Epoch 16 - train_loss: 0.07379  val_loss: 1.21000  val_acc: 0.76641  elapsed: 81m 5s\n================ ༼ つ ◕_◕ ༽つ Epoch 17 - train_loss: 0.10559  val_loss: 1.24898  val_acc: 0.76641  elapsed: 82m 30s\n================ ༼ つ ◕_◕ ༽つ Epoch 18 - train_loss: 0.06274  val_loss: 1.20185  val_acc: 0.76641  elapsed: 83m 56s\n================ ༼ つ ◕_◕ ༽つ Epoch 19 - train_loss: 0.05998  val_loss: 1.25338  val_acc: 0.76641  elapsed: 85m 22s\n================ ༼ つ ◕_◕ ༽つ Epoch 20 - train_loss: 0.04911  val_loss: 1.22630  val_acc: 0.76641  elapsed: 86m 48s\n============== ༼ つ ◕_◕ ༽つ BEST epoch : 20, Accuracy : 0.7664092664092664 ====================================\n========================== ༼ つ ◕_◕ ༽つ Model Load 2_th FOLD =================================\npredict values check :  [-0.81744957 -1.65529895 -1.04801464  0.11835536  3.89325619]\n================ ༼ つ ◕_◕ ༽つ 4/5 fold training starts!\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 1 - train_loss: 1.24524  val_loss: 0.88960  val_acc: 0.68340  elapsed: 88m 21s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 2 - train_loss: 0.81533  val_loss: 0.76428  val_acc: 0.73166  elapsed: 89m 47s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 3 - train_loss: 0.56177  val_loss: 0.70380  val_acc: 0.77027  elapsed: 91m 16s\n================ ༼ つ ◕_◕ ༽つ Epoch 4 - train_loss: 0.41233  val_loss: 0.73184  val_acc: 0.77027  elapsed: 92m 45s\n================ ༼ つ ◕_◕ ༽つ Epoch 5 - train_loss: 0.28578  val_loss: 0.88065  val_acc: 0.77027  elapsed: 94m 11s\n================ ༼ つ ◕_◕ ༽つ Epoch 6 - train_loss: 0.22031  val_loss: 0.87194  val_acc: 0.77027  elapsed: 95m 37s\n================ ༼ つ ◕_◕ ༽つ Epoch 7 - train_loss: 0.15033  val_loss: 0.89508  val_acc: 0.77027  elapsed: 97m 2s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 8 - train_loss: 0.14656  val_loss: 0.98294  val_acc: 0.77220  elapsed: 98m 28s\n================ ༼ つ ◕_◕ ༽つ Epoch 9 - train_loss: 0.10391  val_loss: 1.02709  val_acc: 0.77220  elapsed: 99m 57s\n================ ༼ つ ◕_◕ ༽つ Epoch 10 - train_loss: 0.10149  val_loss: 1.04623  val_acc: 0.77220  elapsed: 101m 23s\n================ ༼ つ ◕_◕ ༽つ Epoch 11 - train_loss: 0.09065  val_loss: 1.11266  val_acc: 0.77220  elapsed: 102m 49s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 12 - train_loss: 0.07500  val_loss: 1.05743  val_acc: 0.77606  elapsed: 104m 15s\n================ ༼ つ ◕_◕ ༽つ Epoch 13 - train_loss: 0.08861  val_loss: 1.16476  val_acc: 0.77606  elapsed: 105m 44s\n================ ༼ つ ◕_◕ ༽つ Epoch 14 - train_loss: 0.08551  val_loss: 1.17960  val_acc: 0.77606  elapsed: 107m 10s\n================ ༼ つ ◕_◕ ༽つ Epoch 15 - train_loss: 0.08847  val_loss: 1.19570  val_acc: 0.77606  elapsed: 108m 35s\n================ ༼ つ ◕_◕ ༽つ Epoch 16 - train_loss: 0.06440  val_loss: 1.07245  val_acc: 0.77606  elapsed: 110m 1s\n================ ༼ つ ◕_◕ ༽つ Epoch 17 - train_loss: 0.06107  val_loss: 1.08461  val_acc: 0.77606  elapsed: 111m 27s\n================ ༼ つ ◕_◕ ༽つ Epoch 18 - train_loss: 0.05304  val_loss: 1.19353  val_acc: 0.77606  elapsed: 112m 53s\n================ ༼ つ ◕_◕ ༽つ Epoch 19 - train_loss: 0.02879  val_loss: 1.30685  val_acc: 0.77606  elapsed: 114m 19s\n================ ༼ つ ◕_◕ ༽つ Epoch 20 - train_loss: 0.05603  val_loss: 1.34832  val_acc: 0.77606  elapsed: 115m 45s\n============== ༼ つ ◕_◕ ༽つ BEST epoch : 20, Accuracy : 0.7760617760617761 ====================================\n========================== ༼ つ ◕_◕ ༽つ Model Load 3_th FOLD =================================\npredict values check :  [-1.54165888 -1.53146303 -2.406703   -0.97923213  6.23691416]\n================ ༼ つ ◕_◕ ༽つ 5/5 fold training starts!\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 1 - train_loss: 1.25060  val_loss: 0.97384  val_acc: 0.62741  elapsed: 117m 18s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 2 - train_loss: 0.83962  val_loss: 0.81059  val_acc: 0.68533  elapsed: 118m 44s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 3 - train_loss: 0.61084  val_loss: 0.75072  val_acc: 0.72587  elapsed: 120m 13s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 4 - train_loss: 0.44675  val_loss: 0.68672  val_acc: 0.75869  elapsed: 121m 41s\n================ ༼ つ ◕_◕ ༽つ Epoch 5 - train_loss: 0.32167  val_loss: 0.91745  val_acc: 0.75869  elapsed: 123m 10s\n================ ༼ つ ◕_◕ ༽つ Epoch 6 - train_loss: 0.26213  val_loss: 0.91354  val_acc: 0.75869  elapsed: 124m 36s\n================ ༼ つ ◕_◕ ༽つ Epoch 7 - train_loss: 0.20572  val_loss: 0.88650  val_acc: 0.75869  elapsed: 126m 2s\n================ ༼ つ ◕_◕ ༽つ Epoch 8 - train_loss: 0.15177  val_loss: 0.94716  val_acc: 0.75869  elapsed: 127m 28s\n================ ༼ つ ◕_◕ ༽つ Epoch 9 - train_loss: 0.11900  val_loss: 1.01129  val_acc: 0.75869  elapsed: 128m 54s\n================ ༼ つ ◕_◕ ༽つ Epoch 10 - train_loss: 0.13787  val_loss: 1.09601  val_acc: 0.75869  elapsed: 130m 20s\n================ ༼ つ ◕_◕ ༽つ Epoch 11 - train_loss: 0.12032  val_loss: 1.14737  val_acc: 0.75869  elapsed: 131m 46s\n================ ༼ つ ◕_◕ ༽つ Epoch 12 - train_loss: 0.08636  val_loss: 1.12600  val_acc: 0.75869  elapsed: 133m 11s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 13 - train_loss: 0.05441  val_loss: 1.12678  val_acc: 0.76255  elapsed: 134m 37s\n================ ༼ つ ◕_◕ ༽つ Epoch 14 - train_loss: 0.04466  val_loss: 1.24464  val_acc: 0.76255  elapsed: 136m 6s\n================ ༼ つ ◕_◕ ༽つ Epoch 15 - train_loss: 0.07197  val_loss: 1.34421  val_acc: 0.76255  elapsed: 137m 32s\n================ ༼ つ ◕_◕ ༽つ Epoch 16 - train_loss: 0.06006  val_loss: 1.23416  val_acc: 0.76255  elapsed: 138m 58s\n================ ༼ つ ◕_◕ ༽つ Epoch 17 - train_loss: 0.07172  val_loss: 1.27671  val_acc: 0.76255  elapsed: 140m 24s\n================ ༼ つ ◕_◕ ༽つ Epoch 18 - train_loss: 0.07090  val_loss: 1.27837  val_acc: 0.76255  elapsed: 141m 49s\n================ ༼ つ ◕_◕ ༽つ Epoch 19 - train_loss: 0.04806  val_loss: 1.36582  val_acc: 0.76255  elapsed: 143m 15s\n================ ༼ つ ◕_◕ ༽つ Epoch 20 - train_loss: 0.04454  val_loss: 1.50119  val_acc: 0.76255  elapsed: 144m 41s\n============== ༼ つ ◕_◕ ༽つ BEST epoch : 20, Accuracy : 0.7625482625482626 ====================================\n========================== ༼ つ ◕_◕ ༽つ Model Load 4_th FOLD =================================\npredict values check :  [-1.09749067 -1.85101628 -1.88581681 -1.1021055   7.09595823]\nCPU times: user 2h 16min 48s, sys: 7min 20s, total: 2h 24min 8s\nWall time: 2h 24min 47s\n"
    }
   ],
   "source": [
    "%%time\n",
    "best_epoch_list = []\n",
    "best_val_acc_list = []\n",
    "start_time = time()\n",
    "n_splits = 5\n",
    "\n",
    "splits = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED).split(x_train, y_train))\n",
    "for fold in [0, 1, 2, 3, 4]:\n",
    "\n",
    "    print(\"================ ༼ つ ◕_◕ ༽つ {}/{} fold training starts!\".format(fold+1, n_splits))\n",
    "    \n",
    "    fold_num = str(fold + 1)\n",
    "\n",
    "    trn_index, val_index = splits[fold]\n",
    "\n",
    "    X_train, X_valid = x_train[trn_index], x_train[val_index]\n",
    "    #train_length, valid_length = lengths[trn_index], lengths[val_index]\n",
    "    Y_train, Y_valid = y_train[trn_index], y_train[val_index]\n",
    "\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype = torch.long), torch.tensor(Y_train, dtype=torch.long)) #TensorDataset(X_train, train_length, torch.tensor(Y_train))\n",
    "    valid_dataset = TensorDataset(torch.tensor(X_valid, dtype = torch.long), torch.tensor(Y_valid, dtype=torch.long)) #TensorDataset(X_valid, valid_length, torch.tensor(Y_valid))\n",
    "    \n",
    "    model = BertForSequenceClassification.from_pretrained(\"../job_nlp/working\",cache_dir=None, num_labels=5)\n",
    "    model.zero_grad()\n",
    "    model = model.to(device)\n",
    "    #optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "    #                 lr=lr,\n",
    "    #                 warmup=0.05,\n",
    "    #                 t_total=num_train_optimization_steps)\n",
    "    #scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    \n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "    #train = train_dataset\n",
    "\n",
    "    num_train_optimization_steps = int(EPOCHS*len(train_dataset)/batch_size/accumulation_steps)\n",
    "\n",
    "    #optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "    #                     lr=lr,\n",
    "    #                     warmup=0.05,\n",
    "    #                     t_total=np.ceil(num_train_optimization_steps))\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr, weight_decay=0.000025)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "    best_valid_score = 0\n",
    "    best_val_acc = 0\n",
    "    \n",
    "    #tq = tqdm_notebook(range(EPOCHS))\n",
    "    #model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\",verbosity=0)\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        \n",
    "        #start_time = time.time()\n",
    "        train_loss = 0\n",
    "        train_total_correct = 0\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        #tk0 = tqdm_notebook(enumerate(train_loader),total=len(train_loader),leave=False)\n",
    "        \n",
    "        for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            preds  = model(x_batch.to(device), attention_mask = (x_batch>0).to(device), labels=None)\n",
    "            loss = criterion(preds, y_batch.to(device))\n",
    "            loss.backward()\n",
    "            \n",
    "            if (i+1) % accumulation_steps == 0:             # Wait for several backward steps\n",
    "                optimizer.step()                            # Now we can do an optimizer step\n",
    "                optimizer.zero_grad()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            train_loss += loss.item()/len(train_loader)\n",
    "            \n",
    "        # Validation Starts\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        valid_total_correct = 0\n",
    "        \n",
    "        #valid_preds = np.zeros(len(valid_dataset),5)\n",
    "        #valid_targets = np.zeros(len(valid_dataset),5)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "                #valid_targets[i*batch_size: (i+1)*batch_size] = y_batch.numpy().copy()\n",
    "                \n",
    "                preds = model(x_batch.to(device), attention_mask = (x_batch>0).to(device), labels=None)\n",
    "                \n",
    "                loss = criterion(preds, y_batch.to(device))\n",
    "                \n",
    "                output_prob = F.softmax(preds, dim=1)\n",
    "\n",
    "                predict_vector = np.argmax(to_numpy(output_prob), axis=1)\n",
    "                label_vector = to_numpy(y_batch)\n",
    "                #valid_preds[i*batch_size: (i+1)*batch_size] = np.argmax(preds_prob.detach().cpu().squeeze().numpy())\n",
    "                bool_vector = predict_vector == label_vector\n",
    "                \n",
    "                val_loss += loss.item()/len(valid_loader)\n",
    "                valid_total_correct += bool_vector.sum()\n",
    "        \n",
    "        #val_score = roc_auc_score(valid_targets, valid_preds)\n",
    "\n",
    "        elapsed = time() - start_time\n",
    "        val_acc = valid_total_correct / len(valid_loader.dataset)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch\n",
    "            print(\"val_acc has improved !! \")\n",
    "            best_epoch_list.append(best_epoch)\n",
    "            best_val_acc_list.append(best_val_acc)\n",
    "            \n",
    "            torch.save(model.state_dict(), '../job_nlp/Bert_content_20e_maxseq400_fold_{}.pt'.format(fold))\n",
    "            #print(\"================ ༼ つ ◕_◕ ༽つ BEST epoch : {}, Accuracy : {} \".format(epoch, best_val_acc))\n",
    "            \n",
    "        #lr = [_['lr'] for _ in optimizer.param_g] # or optimizer\n",
    "        print(\"================ ༼ つ ◕_◕ ༽つ Epoch {} - train_loss: {:.5f}  val_loss: {:.5f}  val_acc: {:.5f}  elapsed: {:.0f}m {:.0f}s\".format(epoch, train_loss, val_loss, best_val_acc, elapsed // 60, elapsed % 60))\n",
    "    print(\"============== ༼ つ ◕_◕ ༽つ BEST epoch : {}, Accuracy : {} ====================================\".format(epoch, best_val_acc))\n",
    "    #best_epoch_list.append(best_epoch)\n",
    "    #best_val_acc_list.append(best_val_acc)\n",
    "    \n",
    "    #---- Inference ----\n",
    "    #batch_size = 8\n",
    "\n",
    "    print(\"========================== ༼ つ ◕_◕ ༽つ Model Load {}_th FOLD =================================\".format(fold))\n",
    "    model.load_state_dict(torch.load('Bert_content_20e_maxseq400_fold_{}.pt'.format(fold)))\n",
    "    model.eval()\n",
    "    predictions = np.zeros((len(test_loader.dataset),5))\n",
    "    with torch.no_grad():\n",
    "        for i, (x_batch, ) in enumerate(test_loader):\n",
    "            preds = model(x_batch.to(device), attention_mask = (x_batch>0).to(device), labels=None)\n",
    "            \n",
    "            predictions[i*batch_size: (i+1)*batch_size] = to_numpy(preds)\n",
    "    print(\"predict values check : \",predictions[0])\n",
    "    np.savetxt(\"../job_nlp/bert_raw_submission/bert_content_20e_maxseq400_fold_{}.csv\".format(fold), predictions, delimiter=\",\")   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}