{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from time import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "import datetime\n",
    "import pkg_resources\n",
    "#import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import gc\n",
    "import re\n",
    "import operator \n",
    "import sys\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, Subset, DataLoader\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "#from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "#%matplotlib inline\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import os\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "import pickle\n",
    "#from apex import amp\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=123):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class AdamW(Optimizer):\n",
    "    \"\"\"Implements AdamW algorithm.\n",
    "\n",
    "    It has been proposed in `Fixing Weight Decay Regularization in Adam`_.\n",
    "\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float, optional): learning rate (default: 1e-3)\n",
    "        betas (Tuple[float, float], optional): coefficients used for computing\n",
    "            running averages of gradient and its square (default: (0.9, 0.999))\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability (default: 1e-8)\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "\n",
    "    .. Fixing Weight Decay Regularization in Adam:\n",
    "    https://arxiv.org/abs/1711.05101\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay)\n",
    "        super(AdamW, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('AdamW does not support sparse gradients, please consider SparseAdam instead')\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                # according to the paper, this penalty should come after the bias correction\n",
    "                # if group['weight_decay'] != 0:\n",
    "                #     grad = grad.add(group['weight_decay'], p.data)\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "\n",
    "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p.data.add_(-group['weight_decay'], p.data)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 295 \n",
    "SEED = 42\n",
    "EPOCHS = 20\n",
    "Data_dir=\"../job_nlp/\"\n",
    "WORK_DIR = \"../job_nlp/working/\"\n",
    "#num_to_load=100000                         #Train size to match time limit\n",
    "#valid_size= 50000                          #Validation Size\n",
    "TARGET = 'smishing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/matsuik/ppbert\n",
    "package_dir_a = \"../job_nlp/ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT\"\n",
    "sys.path.insert(0, package_dir_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/home/yilgukseo/anaconda3/envs/pytorch/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n  return f(*args, **kwds)\n/home/yilgukseo/anaconda3/envs/pytorch/lib/python3.7/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n  return f(*args, **kwds)\n/home/yilgukseo/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py:5879: ResourceWarning: unclosed file <_io.TextIOWrapper name='/home/yilgukseo/.keras/keras.json' mode='r' encoding='UTF-8'>\n  _config = json.load(open(_config_path))\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
    }
   ],
   "source": [
    "from pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification,BertAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "erNorm/beta with shape [768]\nLoading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [768]\nLoading TF weight bert/encoder/layer_9/output/dense/bias with shape [768]\nLoading TF weight bert/encoder/layer_9/output/dense/kernel with shape [3072, 768]\nLoading TF weight bert/pooler/dense/bias with shape [768]\nLoading TF weight bert/pooler/dense/kernel with shape [768, 768]\nLoading TF weight cls/predictions/output_bias with shape [30522]\nLoading TF weight cls/predictions/transform/LayerNorm/beta with shape [768]\nLoading TF weight cls/predictions/transform/LayerNorm/gamma with shape [768]\nLoading TF weight cls/predictions/transform/dense/bias with shape [768]\nLoading TF weight cls/predictions/transform/dense/kernel with shape [768, 768]\nLoading TF weight cls/seq_relationship/output_bias with shape [2]\nLoading TF weight cls/seq_relationship/output_weights with shape [2, 768]\nInitialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\nInitialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\nInitialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\nInitialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\nInitialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\nInitialize PyTorch weight ['cls', 'predictions', 'output_bias']\nInitialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'beta']\nInitialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'gamma']\nInitialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'bias']\nInitialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'kernel']\nInitialize PyTorch weight ['cls', 'seq_relationship', 'output_bias']\nInitialize PyTorch weight ['cls', 'seq_relationship', 'output_weights']\nSave PyTorch model to ../job_nlp/working/pytorch_model.bin\n"
    },
    {
     "data": {
      "text/plain": "'../job_nlp/working/bert_config.json'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Translate model from tensorflow to pytorch\n",
    "BERT_MODEL_PATH = '../job_nlp/bert-pretrained-models/uncased_L-12_H-768_A-12/uncased_L-12_H-768_A-12/'\n",
    "convert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n",
    "    BERT_MODEL_PATH + 'bert_model.ckpt',\n",
    "BERT_MODEL_PATH + 'bert_config.json',\n",
    "WORK_DIR + 'pytorch_model.bin')\n",
    "\n",
    "shutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', WORK_DIR + 'bert_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the Bert configuration file\n",
    "from pytorch_pretrained_bert import BertConfig\n",
    "\n",
    "bert_config = BertConfig('../job_nlp/bert-pretrained-models/uncased_L-12_H-768_A-12/uncased_L-12_H-768_A-12/'+'bert_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"max_position_embeddings\": 512,\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 30522\n}"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the lines to BERT format\n",
    "# Thanks to https://www.kaggle.com/httpwwwfszyc/bert-in-keras-taming\n",
    "def convert_lines(example, max_seq_length,tokenizer):\n",
    "    max_seq_length -=2\n",
    "    all_tokens = []\n",
    "    longer = 0\n",
    "    for text in tqdm_notebook(example):\n",
    "        tokens_a = tokenizer.tokenize(text)\n",
    "        if len(tokens_a)>max_seq_length:\n",
    "            tokens_a = tokens_a[:max_seq_length]\n",
    "            longer += 1\n",
    "        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n",
    "        all_tokens.append(one_token)\n",
    "    print(longer)\n",
    "    return np.array(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_PATH = '../job_nlp/bert-pretrained-models/uncased_L-12_H-768_A-12/uncased_L-12_H-768_A-12/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 31.9 ms, sys: 3.94 ms, total: 35.8 ms\nWall time: 35.4 ms\n"
    }
   ],
   "source": [
    "%%time\n",
    "train_df = pd.read_csv(os.path.join(Data_dir,\"train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(Data_dir,\"test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "3    0.354552\n5    0.248457\n4    0.184414\n2    0.113812\n1    0.098765\nName: label, dtype: float64"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "title      [$$$] 재귀함수를 처리하면서 setTimeout으로 딜레이를 주면 읽는 순서가 ...\ncontent                                                  NaN\nlabel                                                      4\nName: 2400, dtype: object\n"
    }
   ],
   "source": [
    "# replace NaN\n",
    "print(train_df.iloc[2400,:])\n",
    "train_df.iloc[2400,1] = \"@@@\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 41.8 ms, sys: 0 ns, total: 41.8 ms\nWall time: 41.4 ms\n"
    }
   ],
   "source": [
    "%%time\n",
    "train_df['text'] = train_df[['title', 'content']].apply(lambda x: ' '.join(x), axis = 1)\n",
    "test_df['text'] = test_df[['title', 'content']].apply(lambda x: ' '.join(x), axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>content</th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>wait()과 sleep()의 차이점은 뭔가요</td>\n      <td>###발생하는 문제 및 실행환경\\nwait()과 sleep()의 차이점은 뭔가요</td>\n      <td>3</td>\n      <td>wait()과 sleep()의 차이점은 뭔가요 ###발생하는 문제 및 실행환경\\nw...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>$$$ 초보 외부 클래스 멤버 변수 사용 질문합니다.ㅠㅠ</td>\n      <td>1.헤더에, 사용할 멤버변수가 담긴 헤더 파일이 Include 되어있습니다.\\n예를...</td>\n      <td>2</td>\n      <td>$$$ 초보 외부 클래스 멤버 변수 사용 질문합니다.ㅠㅠ 1.헤더에, 사용할 멤버변...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ORM: Sequelize: 다대다 관계 쿼리</td>\n      <td>안녕하세요.\\n\\n어떻게 다대다 관계 쿼리를 해야하나요? 예를들어, `product...</td>\n      <td>4</td>\n      <td>ORM: Sequelize: 다대다 관계 쿼리 안녕하세요.\\n\\n어떻게 다대다 관계...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>$$$에서 숫자가 NaN인지 검사하려면 어떻게해야하죠?</td>\n      <td>```\\nparseFloat('geoff') == NaN;\\n\\nparseFloat...</td>\n      <td>4</td>\n      <td>$$$에서 숫자가 NaN인지 검사하려면 어떻게해야하죠? ```\\nparseFloat...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>$$$ 2.7에서 중국어, 특수문자 인코딩 하는 방법이 뭔가요?</td>\n      <td>```\\n&gt;&gt;&gt;dict['name']\\n胡安·马塔\\n&gt;&gt;&gt;json.dumps(dic...</td>\n      <td>5</td>\n      <td>$$$ 2.7에서 중국어, 특수문자 인코딩 하는 방법이 뭔가요? ```\\n&gt;&gt;&gt;di...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                 title  \\\n0            wait()과 sleep()의 차이점은 뭔가요   \n1      $$$ 초보 외부 클래스 멤버 변수 사용 질문합니다.ㅠㅠ   \n2            ORM: Sequelize: 다대다 관계 쿼리   \n3       $$$에서 숫자가 NaN인지 검사하려면 어떻게해야하죠?   \n4  $$$ 2.7에서 중국어, 특수문자 인코딩 하는 방법이 뭔가요?   \n\n                                             content  label  \\\n0       ###발생하는 문제 및 실행환경\\nwait()과 sleep()의 차이점은 뭔가요      3   \n1  1.헤더에, 사용할 멤버변수가 담긴 헤더 파일이 Include 되어있습니다.\\n예를...      2   \n2  안녕하세요.\\n\\n어떻게 다대다 관계 쿼리를 해야하나요? 예를들어, `product...      4   \n3  ```\\nparseFloat('geoff') == NaN;\\n\\nparseFloat...      4   \n4  ```\\n>>>dict['name']\\n胡安·马塔\\n>>>json.dumps(dic...      5   \n\n                                                text  \n0  wait()과 sleep()의 차이점은 뭔가요 ###발생하는 문제 및 실행환경\\nw...  \n1  $$$ 초보 외부 클래스 멤버 변수 사용 질문합니다.ㅠㅠ 1.헤더에, 사용할 멤버변...  \n2  ORM: Sequelize: 다대다 관계 쿼리 안녕하세요.\\n\\n어떻게 다대다 관계...  \n3  $$$에서 숫자가 NaN인지 검사하려면 어떻게해야하죠? ```\\nparseFloat...  \n4  $$$ 2.7에서 중국어, 특수문자 인코딩 하는 방법이 뭔가요? ```\\n>>>di...  "
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'```\\n>>>dict[\\'name\\']\\n胡安·马塔\\n>>>json.dumps(dict[\\'name\\']).replace(\"\\\\\\\\\",\"\\\\\\\\\\\\\\\\\")\\n\"\\\\\\\\u80e1\\\\\\\\u5b89\\\\\\\\u00b7\\\\\\\\u9a6c\\\\\\\\u5854\"\\n>>>\"Player name is \\'{}\\'\".format(dict[\\'name\\'])\\nUnicodeEncodeError \\'ascii\\' codec can\\'t encode character\\n>>>\"Player name is \\'{}\\'\".format(json.dumps(dict[\\'name\\']))\\nUnicodeEncodeError \\'ascii\\' codec can\\'t encode character\\n```\\n다른 중국 단어들은 잘 들어가는데  胡安·马塔 이 단어에서 인코드 에러가 발생하네요... 중국어가 문제가 아니라 가운데 점이 문제인거같은데 혹시 이 경우 어떻게 포멧팅 해야하나요?\\n'"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[4,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "<>:17: DeprecationWarning: invalid escape sequence \\[\n"
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \n",
    "#     stopwords = ['XXX', '.', '을', '를', '이', '가', '-', '(', ')', ':', '!', '?', ')-', '.-', 'ㅡ', 'XXXXXX', '..', '.(', '은', '는']\n",
    "#     text = re.sub(\".\", \" \", text)\n",
    "    #text = re.sub(\"\", \" \", text)\n",
    "    #text = re.sub(\"XXXXXX\", \" \", text)\n",
    "    text = re.sub(\"[^ .?!/@$%~|0-9|ㄱ-ㅣ가-힣]+\", \"\", text) # 한글과 띄어쓰기, 특수기호 일부를 제외한 모든 글자\n",
    "    #text = re.sub(\"[\\s]+\", \"\", text.strip()) # white space duplicate\n",
    "    #text = re.sub(\"\\s+\", \"\", text.strip()) # white space duplicate\n",
    "#     text = re.sub(\"[\\.]+\", \"\", text.strip()) # full stop duplicate\n",
    "    \n",
    "    text = re.sub(\"(?s)<ref>.+?</ref>\", \"\", text) # remove reference links\n",
    "    text = re.sub(\"(?s)<[^>]+>\", \"\", text) # remove html tags\n",
    "    text = re.sub(\"&[a-z]+;\", \"\", text) # remove html entities\n",
    "    text = re.sub(\"(?s){{.+?}}\", \"\", text) # remove markup tags\n",
    "    text = re.sub(\"(?s){.+?}\", \"\", text) # remove markup tags\n",
    "    text = re.sub(\"(?s)\\[\\[([^]]+\\|)\", \"\", text) # remove link target strings\n",
    "    text = re.sub(\"(?s)\\[\\[([^]]+\\:.+?]])\", \"\", text) # remove media links\n",
    "    \n",
    "    text = re.sub(\"[']{5}\", \"\", text) # remove italic+bold symbols\n",
    "    text = re.sub(\"[']{3}\", \"\", text) # remove bold symbols\n",
    "    text = re.sub(\"[']{2}\", \"\", text) # remove italic symbols\n",
    "    text = re.sub(r'\\d+', ' ', text) # clean numbers\n",
    "    \n",
    "#     text = re.sub(r\"[^ \\r\\n\\p{Hangul}.?!]\", \" \", text) # Replace unacceptable characters with a space.\n",
    "#     text = re.sub(\"[ ]{2,}\", \" \", text) # Squeeze spaces.\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 160 ms, sys: 0 ns, total: 160 ms\nWall time: 159 ms\n"
    }
   ],
   "source": [
    "%%time\n",
    "train_df['clean_text'] = train_df['text'].apply(lambda x : preprocess_text(x))\n",
    "test_df['clean_text'] = test_df['text'].apply(lambda x : preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'$$$  . 에서 중국어 특수문자 인코딩 하는 방법이 뭔가요? ..    .        ..     다른 중국 단어들은 잘 들어가는데   이 단어에서 인코드 에러가 발생하네요... 중국어가 문제가 아니라 가운데 점이 문제인거같은데 혹시 이 경우 어떻게 포멧팅 해야하나요?'"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[4,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\"$$$으로 탐색기 시작위치 변경하고싶습니다. $$$으로 코딩중인데\\n\\n    def explorer():\\n\\n     subprocess.call('explorer')\\n\\n으로 탐색기를 실행하고\\n file의 경로가 뭘 선택하냐에 따라서 달라져서 절대값으로 경로설정은 힘들고\\n\\n    path=os.path.dirname(file) \\n으로 파일 경로 구해서\\n\\n탐색기 맨처음 시작하는 위치를 path로 지정하고 싶습니다.\\n\\n이럴땐 어떻게 해야하나요? \\n\""
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[5,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'$$$으로 탐색기 시작위치 변경하고싶습니다. $$$으로 코딩중인데          .으로 탐색기를 실행하고 의 경로가 뭘 선택하냐에 따라서 달라져서 절대값으로 경로설정은 힘들고    .. 으로 파일 경로 구해서탐색기 맨처음 시작하는 위치를 로 지정하고 싶습니다.이럴땐 어떻게 해야하나요? '"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[5,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>content</th>\n      <th>label</th>\n      <th>text</th>\n      <th>clean_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>wait()과 sleep()의 차이점은 뭔가요</td>\n      <td>###발생하는 문제 및 실행환경\\nwait()과 sleep()의 차이점은 뭔가요</td>\n      <td>3</td>\n      <td>wait()과 sleep()의 차이점은 뭔가요 ###발생하는 문제 및 실행환경\\nw...</td>\n      <td>과 의 차이점은 뭔가요 발생하는 문제 및 실행환경과 의 차이점은 뭔가요</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>$$$ 초보 외부 클래스 멤버 변수 사용 질문합니다.ㅠㅠ</td>\n      <td>1.헤더에, 사용할 멤버변수가 담긴 헤더 파일이 Include 되어있습니다.\\n예를...</td>\n      <td>2</td>\n      <td>$$$ 초보 외부 클래스 멤버 변수 사용 질문합니다.ㅠㅠ 1.헤더에, 사용할 멤버변...</td>\n      <td>$$$ 초보 외부 클래스 멤버 변수 사용 질문합니다.ㅠㅠ  .헤더에 사용할 멤버변수...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ORM: Sequelize: 다대다 관계 쿼리</td>\n      <td>안녕하세요.\\n\\n어떻게 다대다 관계 쿼리를 해야하나요? 예를들어, `product...</td>\n      <td>4</td>\n      <td>ORM: Sequelize: 다대다 관계 쿼리 안녕하세요.\\n\\n어떻게 다대다 관계...</td>\n      <td>다대다 관계 쿼리 안녕하세요.어떻게 다대다 관계 쿼리를 해야하나요? 예를들어  ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>$$$에서 숫자가 NaN인지 검사하려면 어떻게해야하죠?</td>\n      <td>```\\nparseFloat('geoff') == NaN;\\n\\nparseFloat...</td>\n      <td>4</td>\n      <td>$$$에서 숫자가 NaN인지 검사하려면 어떻게해야하죠? ```\\nparseFloat...</td>\n      <td>$$$에서 숫자가 인지 검사하려면 어떻게해야하죠?     .이렇게 해봤는데 둘다 로...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>$$$ 2.7에서 중국어, 특수문자 인코딩 하는 방법이 뭔가요?</td>\n      <td>```\\n&gt;&gt;&gt;dict['name']\\n胡安·马塔\\n&gt;&gt;&gt;json.dumps(dic...</td>\n      <td>5</td>\n      <td>$$$ 2.7에서 중국어, 특수문자 인코딩 하는 방법이 뭔가요? ```\\n&gt;&gt;&gt;di...</td>\n      <td>$$$  . 에서 중국어 특수문자 인코딩 하는 방법이 뭔가요? ..    .    ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                 title  \\\n0            wait()과 sleep()의 차이점은 뭔가요   \n1      $$$ 초보 외부 클래스 멤버 변수 사용 질문합니다.ㅠㅠ   \n2            ORM: Sequelize: 다대다 관계 쿼리   \n3       $$$에서 숫자가 NaN인지 검사하려면 어떻게해야하죠?   \n4  $$$ 2.7에서 중국어, 특수문자 인코딩 하는 방법이 뭔가요?   \n\n                                             content  label  \\\n0       ###발생하는 문제 및 실행환경\\nwait()과 sleep()의 차이점은 뭔가요      3   \n1  1.헤더에, 사용할 멤버변수가 담긴 헤더 파일이 Include 되어있습니다.\\n예를...      2   \n2  안녕하세요.\\n\\n어떻게 다대다 관계 쿼리를 해야하나요? 예를들어, `product...      4   \n3  ```\\nparseFloat('geoff') == NaN;\\n\\nparseFloat...      4   \n4  ```\\n>>>dict['name']\\n胡安·马塔\\n>>>json.dumps(dic...      5   \n\n                                                text  \\\n0  wait()과 sleep()의 차이점은 뭔가요 ###발생하는 문제 및 실행환경\\nw...   \n1  $$$ 초보 외부 클래스 멤버 변수 사용 질문합니다.ㅠㅠ 1.헤더에, 사용할 멤버변...   \n2  ORM: Sequelize: 다대다 관계 쿼리 안녕하세요.\\n\\n어떻게 다대다 관계...   \n3  $$$에서 숫자가 NaN인지 검사하려면 어떻게해야하죠? ```\\nparseFloat...   \n4  $$$ 2.7에서 중국어, 특수문자 인코딩 하는 방법이 뭔가요? ```\\n>>>di...   \n\n                                          clean_text  \n0            과 의 차이점은 뭔가요 발생하는 문제 및 실행환경과 의 차이점은 뭔가요  \n1  $$$ 초보 외부 클래스 멤버 변수 사용 질문합니다.ㅠㅠ  .헤더에 사용할 멤버변수...  \n2    다대다 관계 쿼리 안녕하세요.어떻게 다대다 관계 쿼리를 해야하나요? 예를들어  ...  \n3  $$$에서 숫자가 인지 검사하려면 어떻게해야하죠?     .이렇게 해봤는데 둘다 로...  \n4  $$$  . 에서 중국어 특수문자 인코딩 하는 방법이 뭔가요? ..    .    ...  "
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "25"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.iloc[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "43"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.iloc[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "69"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.iloc[0,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 108 ms, sys: 0 ns, total: 108 ms\nWall time: 108 ms\n<unknown>:8: DeprecationWarning: invalid escape sequence \\s\n"
    }
   ],
   "source": [
    "%%time\n",
    "def remove_space(text):\n",
    "    \"\"\"\n",
    "    remove extra spaces and ending space if any\n",
    "    \"\"\"\n",
    "    for space in spaces:\n",
    "        text = text.replace(space, ' ')\n",
    "    text = text.strip()\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "spaces = ['\\u200b', '\\u200e', '\\u202a', '\\u202c', '\\ufeff', '\\uf0d8', '\\u2061', '\\x10', '\\x7f', '\\x9d', '\\xad', '\\xa0']\n",
    "\n",
    "train_df['clean_space_only_text'] = train_df['text'].apply(lambda x: remove_space(x))\n",
    "test_df['clean_space_only_text'] = test_df['text'].apply(lambda x: remove_space(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['t_length'] = train_df['title'].apply(lambda x: len(x))\n",
    "train_df['c_length'] = train_df['content'].apply(lambda x: len(x))\n",
    "train_df['text_length'] = train_df['text'].apply(lambda x: len(x))\n",
    "train_df['clean_text_length'] = train_df['clean_text'].apply(lambda x: len(x))\n",
    "train_df['clean_space_only_text_length'] = train_df['clean_space_only_text'].apply(lambda x: len(x))\n",
    "\n",
    "test_df['t_length'] = test_df['title'].apply(lambda x: len(x))\n",
    "test_df['c_length'] = test_df['content'].apply(lambda x: len(x))\n",
    "test_df['text_length'] = test_df['text'].apply(lambda x: len(x))\n",
    "test_df['clean_text_length'] = test_df['clean_text'].apply(lambda x: len(x))\n",
    "test_df['clean_space_only_text_length'] = test_df['clean_space_only_text'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>t_length</th>\n      <th>c_length</th>\n      <th>text_length</th>\n      <th>clean_text_length</th>\n      <th>clean_space_only_text_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>2592.000000</td>\n      <td>2592.000000</td>\n      <td>2592.000000</td>\n      <td>2592.000000</td>\n      <td>2592.000000</td>\n      <td>2592.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>3.369985</td>\n      <td>29.945988</td>\n      <td>891.706404</td>\n      <td>922.652392</td>\n      <td>399.340664</td>\n      <td>772.735725</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.245321</td>\n      <td>15.235909</td>\n      <td>1684.607332</td>\n      <td>1685.667323</td>\n      <td>634.178904</td>\n      <td>1328.001201</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>3.000000</td>\n      <td>8.000000</td>\n      <td>8.000000</td>\n      <td>8.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>3.000000</td>\n      <td>19.000000</td>\n      <td>180.000000</td>\n      <td>208.000000</td>\n      <td>127.000000</td>\n      <td>201.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>3.000000</td>\n      <td>27.000000</td>\n      <td>385.000000</td>\n      <td>418.500000</td>\n      <td>222.000000</td>\n      <td>390.500000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>4.000000</td>\n      <td>37.000000</td>\n      <td>896.000000</td>\n      <td>920.750000</td>\n      <td>408.000000</td>\n      <td>804.250000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>5.000000</td>\n      <td>119.000000</td>\n      <td>28756.000000</td>\n      <td>28780.000000</td>\n      <td>7744.000000</td>\n      <td>26799.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "             label     t_length      c_length   text_length  \\\ncount  2592.000000  2592.000000   2592.000000   2592.000000   \nmean      3.369985    29.945988    891.706404    922.652392   \nstd       1.245321    15.235909   1684.607332   1685.667323   \nmin       1.000000     3.000000      3.000000      8.000000   \n25%       3.000000    19.000000    180.000000    208.000000   \n50%       3.000000    27.000000    385.000000    418.500000   \n75%       4.000000    37.000000    896.000000    920.750000   \nmax       5.000000   119.000000  28756.000000  28780.000000   \n\n       clean_text_length  clean_space_only_text_length  \ncount        2592.000000                   2592.000000  \nmean          399.340664                    772.735725  \nstd           634.178904                   1328.001201  \nmin             8.000000                      8.000000  \n25%           127.000000                    201.000000  \n50%           222.000000                    390.500000  \n75%           408.000000                    804.250000  \nmax          7744.000000                  26799.000000  "
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>t_length</th>\n      <th>c_length</th>\n      <th>text_length</th>\n      <th>clean_text_length</th>\n      <th>clean_space_only_text_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>500.000000</td>\n      <td>500.000000</td>\n      <td>500.000000</td>\n      <td>500.000000</td>\n      <td>500.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>29.310000</td>\n      <td>858.270000</td>\n      <td>888.580000</td>\n      <td>431.228000</td>\n      <td>707.678000</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>14.786046</td>\n      <td>1957.436752</td>\n      <td>1956.747171</td>\n      <td>1341.237312</td>\n      <td>975.193013</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>5.000000</td>\n      <td>20.000000</td>\n      <td>53.000000</td>\n      <td>31.000000</td>\n      <td>50.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>19.750000</td>\n      <td>192.250000</td>\n      <td>227.250000</td>\n      <td>131.000000</td>\n      <td>219.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>26.500000</td>\n      <td>371.000000</td>\n      <td>399.000000</td>\n      <td>211.000000</td>\n      <td>376.500000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>36.000000</td>\n      <td>841.250000</td>\n      <td>870.500000</td>\n      <td>378.250000</td>\n      <td>770.250000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>134.000000</td>\n      <td>35210.000000</td>\n      <td>35226.000000</td>\n      <td>27594.000000</td>\n      <td>9369.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "         t_length      c_length   text_length  clean_text_length  \\\ncount  500.000000    500.000000    500.000000         500.000000   \nmean    29.310000    858.270000    888.580000         431.228000   \nstd     14.786046   1957.436752   1956.747171        1341.237312   \nmin      5.000000     20.000000     53.000000          31.000000   \n25%     19.750000    192.250000    227.250000         131.000000   \n50%     26.500000    371.000000    399.000000         211.000000   \n75%     36.000000    841.250000    870.500000         378.250000   \nmax    134.000000  35210.000000  35226.000000       27594.000000   \n\n       clean_space_only_text_length  \ncount                    500.000000  \nmean                     707.678000  \nstd                      975.193013  \nmin                       50.000000  \n25%                      219.000000  \n50%                      376.500000  \n75%                      770.250000  \nmax                     9369.000000  "
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\"$$$으로 탐색기 시작위치 변경하고싶습니다. $$$으로 코딩중인데\\n\\n    def explorer():\\n\\n     subprocess.call('explorer')\\n\\n으로 탐색기를 실행하고\\n file의 경로가 뭘 선택하냐에 따라서 달라져서 절대값으로 경로설정은 힘들고\\n\\n    path=os.path.dirname(file) \\n으로 파일 경로 구해서\\n\\n탐색기 맨처음 시작하는 위치를 path로 지정하고 싶습니다.\\n\\n이럴땐 어떻게 해야하나요? \\n\""
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[5,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'$$$으로 탐색기 시작위치 변경하고싶습니다. $$$으로 코딩중인데          .으로 탐색기를 실행하고 의 경로가 뭘 선택하냐에 따라서 달라져서 절대값으로 경로설정은 힘들고    .. 으로 파일 경로 구해서탐색기 맨처음 시작하는 위치를 로 지정하고 싶습니다.이럴땐 어떻게 해야하나요? '"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[5,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\"$$$으로 탐색기 시작위치 변경하고싶습니다. $$$으로 코딩중인데 def explorer(): subprocess.call('explorer') 으로 탐색기를 실행하고 file의 경로가 뭘 선택하냐에 따라서 달라져서 절대값으로 경로설정은 힘들고 path=os.path.dirname(file) 으로 파일 경로 구해서 탐색기 맨처음 시작하는 위치를 path로 지정하고 싶습니다. 이럴땐 어떻게 해야하나요?\""
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[5,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'$$$  . 에서 중국어 특수문자 인코딩 하는 방법이 뭔가요? ..    .        ..     다른 중국 단어들은 잘 들어가는데   이 단어에서 인코드 에러가 발생하네요... 중국어가 문제가 아니라 가운데 점이 문제인거같은데 혹시 이 경우 어떻게 포멧팅 해야하나요?'"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[4,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'$$$ 2.7에서 중국어, 특수문자 인코딩 하는 방법이 뭔가요? ``` >>>dict[\\'name\\'] 胡安·马塔 >>>json.dumps(dict[\\'name\\']).replace(\"\\\\\\\\\",\"\\\\\\\\\\\\\\\\\") \"\\\\\\\\u80e1\\\\\\\\u5b89\\\\\\\\u00b7\\\\\\\\u9a6c\\\\\\\\u5854\" >>>\"Player name is \\'{}\\'\".format(dict[\\'name\\']) UnicodeEncodeError \\'ascii\\' codec can\\'t encode character >>>\"Player name is \\'{}\\'\".format(json.dumps(dict[\\'name\\'])) UnicodeEncodeError \\'ascii\\' codec can\\'t encode character ``` 다른 중국 단어들은 잘 들어가는데 胡安·马塔 이 단어에서 인코드 에러가 발생하네요... 중국어가 문제가 아니라 가운데 점이 문제인거같은데 혹시 이 경우 어떻게 포멧팅 해야하나요?'"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "loaded 2592 records\n/home/yilgukseo/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  import sys\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4951758237ec41d7ab68b67e6dd45b94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=2592.0), HTML(value='')))"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n522\nX_train : 2592\nloaded 500 records\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5d90dc48184fc4b91e110d8fa5ce84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=500.0), HTML(value='')))"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n97\nX_test : 500\n"
    }
   ],
   "source": [
    "#%%time\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)\n",
    "#train_df = pd.read_csv(os.path.join(Data_dir,\"train.csv\"))#.sample(num_to_load+valid_size,random_state=SEED)\n",
    "print('loaded %d records' % len(train_df))\n",
    "# Make sure all comment_text values are strings\n",
    "train_df['clean_text'] = train_df['clean_text'].astype(str) \n",
    "x_train = convert_lines(train_df[\"clean_text\"].fillna(\"DUMMY_VALUE\"),MAX_SEQUENCE_LENGTH,tokenizer)\n",
    "print(\"X_train : {}\".format(len(x_train)))\n",
    "\n",
    "#test_df = pd.read_csv(os.path.join(Data_dir,\"public_test.csv\"))#.sample(num_to_load+valid_size,random_state=SEED)\n",
    "print('loaded %d records' % len(test_df))\n",
    "test_df['clean_text'] = test_df['clean_text'].astype(str) \n",
    "x_test = convert_lines(test_df[\"clean_text\"].fillna(\"DUMMY_VALUE\"),MAX_SEQUENCE_LENGTH,tokenizer)\n",
    "print(\"X_test : {}\".format(len(x_test)))\n",
    "\n",
    "train_df=train_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# above not working in linux ?? these x_train & x_test are obtained from windows\n",
    "#x_train = np.loadtxt('../job_nlp/x_train.csv', delimiter=',')\n",
    "#x_test = np.loadtxt('../job_nlp/x_test.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(SEED)\n",
    "output_model_file = \"bert_pytorch.bin\"\n",
    "\n",
    "lr=2e-5\n",
    "batch_size = 8\n",
    "accumulation_steps=2\n",
    "n_labels = 2\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "TARGET = 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[TARGET] = train_df[TARGET]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([2, 1, 3,  ..., 0, 4, 1])"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x_train = train_df['text']\n",
    "y_train = torch.tensor(train_df[TARGET])#.long()\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([2, 1, 3, 3, 4])"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(x):\n",
    "    return x.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(torch.tensor(x_test, dtype = torch.long)) #TensorDataset(X_valid, valid_length, torch.tensor(Y_valid))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"../job_nlp/working\",cache_dir=None, num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "================ ༼ つ ◕_◕ ༽つ 1/5 fold training starts!\n/home/yilgukseo/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 1 - train_loss: 1.52010  val_loss: 1.47821  val_acc: 0.36031  elapsed: 1m 28s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 2 - train_loss: 1.45455  val_loss: 1.40272  val_acc: 0.42197  elapsed: 2m 55s\n================ ༼ つ ◕_◕ ༽つ Epoch 3 - train_loss: 1.38622  val_loss: 1.34463  val_acc: 0.42197  elapsed: 4m 24s\n================ ༼ つ ◕_◕ ༽つ Epoch 4 - train_loss: 1.31471  val_loss: 1.41369  val_acc: 0.42197  elapsed: 5m 50s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 5 - train_loss: 1.27063  val_loss: 1.31701  val_acc: 0.44894  elapsed: 7m 15s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 6 - train_loss: 1.16979  val_loss: 1.28135  val_acc: 0.47399  elapsed: 8m 44s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 7 - train_loss: 1.10931  val_loss: 1.30167  val_acc: 0.49711  elapsed: 10m 13s\nval_acc has improved !! \n================ ༼ つ ◕_◕ ༽つ Epoch 8 - train_loss: 1.02480  val_loss: 1.24967  val_acc: 0.52794  elapsed: 11m 42s\n================ ༼ つ ◕_◕ ༽つ Epoch 9 - train_loss: 0.91625  val_loss: 1.28273  val_acc: 0.52794  elapsed: 13m 12s\n================ ༼ つ ◕_◕ ༽つ Epoch 10 - train_loss: 0.83104  val_loss: 1.31782  val_acc: 0.52794  elapsed: 14m 38s\n================ ༼ つ ◕_◕ ༽つ Epoch 11 - train_loss: 0.69551  val_loss: 1.51043  val_acc: 0.52794  elapsed: 16m 3s\n================ ༼ つ ◕_◕ ༽つ Epoch 12 - train_loss: 0.60061  val_loss: 1.51910  val_acc: 0.52794  elapsed: 17m 29s\n================ ༼ つ ◕_◕ ༽つ Epoch 13 - train_loss: 0.51014  val_loss: 1.67497  val_acc: 0.52794  elapsed: 18m 55s\n================ ༼ つ ◕_◕ ༽つ Epoch 14 - train_loss: 0.46987  val_loss: 1.68025  val_acc: 0.52794  elapsed: 20m 21s\n================ ༼ つ ◕_◕ ༽つ Epoch 15 - train_loss: 0.35554  val_loss: 1.86582  val_acc: 0.52794  elapsed: 21m 47s\n================ ༼ つ ◕_◕ ༽つ Epoch 16 - train_loss: 0.35826  val_loss: 1.91841  val_acc: 0.52794  elapsed: 23m 13s\n================ ༼ つ ◕_◕ ༽つ Epoch 17 - train_loss: 0.23737  val_loss: 2.07564  val_acc: 0.52794  elapsed: 24m 39s\n================ ༼ つ ◕_◕ ༽つ Epoch 18 - train_loss: 0.19133  val_loss: 2.12797  val_acc: 0.52794  elapsed: 26m 5s\n"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_epoch_list = []\n",
    "best_val_acc_list = []\n",
    "start_time = time()\n",
    "n_splits = 5\n",
    "\n",
    "splits = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED).split(x_train, y_train))\n",
    "for fold in [0, 1, 2, 3, 4]:\n",
    "\n",
    "    print(\"================ ༼ つ ◕_◕ ༽つ {}/{} fold training starts!\".format(fold+1, n_splits))\n",
    "    \n",
    "    fold_num = str(fold + 1)\n",
    "\n",
    "    trn_index, val_index = splits[fold]\n",
    "\n",
    "    X_train, X_valid = x_train[trn_index], x_train[val_index]\n",
    "    #train_length, valid_length = lengths[trn_index], lengths[val_index]\n",
    "    Y_train, Y_valid = y_train[trn_index], y_train[val_index]\n",
    "\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype = torch.long), torch.tensor(Y_train, dtype=torch.long)) #TensorDataset(X_train, train_length, torch.tensor(Y_train))\n",
    "    valid_dataset = TensorDataset(torch.tensor(X_valid, dtype = torch.long), torch.tensor(Y_valid, dtype=torch.long)) #TensorDataset(X_valid, valid_length, torch.tensor(Y_valid))\n",
    "    \n",
    "    model = BertForSequenceClassification.from_pretrained(\"../job_nlp/working\",cache_dir=None, num_labels=5)\n",
    "    model.zero_grad()\n",
    "    model = model.to(device)\n",
    "    #optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "    #                 lr=lr,\n",
    "    #                 warmup=0.05,\n",
    "    #                 t_total=num_train_optimization_steps)\n",
    "    #scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    \n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "    #train = train_dataset\n",
    "\n",
    "    num_train_optimization_steps = int(EPOCHS*len(train_dataset)/batch_size/accumulation_steps)\n",
    "\n",
    "    #optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "    #                     lr=lr,\n",
    "    #                     warmup=0.05,\n",
    "    #                     t_total=np.ceil(num_train_optimization_steps))\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr, weight_decay=0.000025)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "    best_valid_score = 0\n",
    "    best_val_acc = 0\n",
    "    \n",
    "    #tq = tqdm_notebook(range(EPOCHS))\n",
    "    #model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\",verbosity=0)\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        \n",
    "        #start_time = time.time()\n",
    "        train_loss = 0\n",
    "        train_total_correct = 0\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        #tk0 = tqdm_notebook(enumerate(train_loader),total=len(train_loader),leave=False)\n",
    "        \n",
    "        for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            preds  = model(x_batch.to(device), attention_mask = (x_batch>0).to(device), labels=None)\n",
    "            loss = criterion(preds, y_batch.to(device))\n",
    "            loss.backward()\n",
    "            \n",
    "            if (i+1) % accumulation_steps == 0:             # Wait for several backward steps\n",
    "                optimizer.step()                            # Now we can do an optimizer step\n",
    "                optimizer.zero_grad()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            train_loss += loss.item()/len(train_loader)\n",
    "            \n",
    "        # Validation Starts\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        valid_total_correct = 0\n",
    "        \n",
    "        #valid_preds = np.zeros(len(valid_dataset),5)\n",
    "        #valid_targets = np.zeros(len(valid_dataset),5)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "                #valid_targets[i*batch_size: (i+1)*batch_size] = y_batch.numpy().copy()\n",
    "                \n",
    "                preds = model(x_batch.to(device), attention_mask = (x_batch>0).to(device), labels=None)\n",
    "                \n",
    "                loss = criterion(preds, y_batch.to(device))\n",
    "                \n",
    "                output_prob = F.softmax(preds, dim=1)\n",
    "\n",
    "                predict_vector = np.argmax(to_numpy(output_prob), axis=1)\n",
    "                label_vector = to_numpy(y_batch)\n",
    "                #valid_preds[i*batch_size: (i+1)*batch_size] = np.argmax(preds_prob.detach().cpu().squeeze().numpy())\n",
    "                bool_vector = predict_vector == label_vector\n",
    "                \n",
    "                val_loss += loss.item()/len(valid_loader)\n",
    "                valid_total_correct += bool_vector.sum()\n",
    "        \n",
    "        #val_score = roc_auc_score(valid_targets, valid_preds)\n",
    "\n",
    "        elapsed = time() - start_time\n",
    "        val_acc = valid_total_correct / len(valid_loader.dataset)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch\n",
    "            print(\"val_acc has improved !! \")\n",
    "            best_epoch_list.append(best_epoch)\n",
    "            best_val_acc_list.append(best_val_acc)\n",
    "            \n",
    "            torch.save(model.state_dict(), '../job_nlp/Bert_clean_text_20e_maxseq400_fold_{}.pt'.format(fold))\n",
    "            #print(\"================ ༼ つ ◕_◕ ༽つ BEST epoch : {}, Accuracy : {} \".format(epoch, best_val_acc))\n",
    "            \n",
    "        #lr = [_['lr'] for _ in optimizer.param_g] # or optimizer\n",
    "        print(\"================ ༼ つ ◕_◕ ༽つ Epoch {} - train_loss: {:.5f}  val_loss: {:.5f}  val_acc: {:.5f}  elapsed: {:.0f}m {:.0f}s\".format(epoch, train_loss, val_loss, best_val_acc, elapsed // 60, elapsed % 60))\n",
    "    print(\"============== ༼ つ ◕_◕ ༽つ BEST epoch : {}, Accuracy : {} ====================================\".format(epoch, best_val_acc))\n",
    "    #best_epoch_list.append(best_epoch)\n",
    "    #best_val_acc_list.append(best_val_acc)\n",
    "    \n",
    "    #---- Inference ----\n",
    "    #batch_size = 8\n",
    "\n",
    "    print(\"========================== ༼ つ ◕_◕ ༽つ Model Load {}_th FOLD =================================\".format(fold))\n",
    "    model.load_state_dict(torch.load('Bert_clean_text_20e_maxseq400_fold_{}.pt'.format(fold)))\n",
    "    model.eval()\n",
    "    predictions = np.zeros((len(test_loader.dataset),5))\n",
    "    with torch.no_grad():\n",
    "        for i, (x_batch, ) in enumerate(test_loader):\n",
    "            preds = model(x_batch.to(device), attention_mask = (x_batch>0).to(device), labels=None)\n",
    "            \n",
    "            predictions[i*batch_size: (i+1)*batch_size] = to_numpy(preds)\n",
    "    print(\"predict values check : \",predictions[0])\n",
    "    np.savetxt(\"../job_nlp/bert_raw_submission/bert_clean_text_20e_maxseq400_fold_{}.csv\".format(fold), predictions, delimiter=\",\")   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}